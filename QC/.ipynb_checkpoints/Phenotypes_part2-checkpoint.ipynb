{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/lustre06/project/6060121/CLSA_PheWeb_shared/Original/23ME002_UdeM_SGTaliun_Baseline/23ME002_UdeM_SGTaliun_Baseline_CoPv7_Qx_PA_BS.csv\"\n",
    "txt_path = \"Binary_good_GWAS_1000cases.txt\" \n",
    "with open(txt_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    lines = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30097, 3840)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>startlanguage_COM</th>\n",
       "      <th>startdate_COM</th>\n",
       "      <th>SEX_ASK_COM</th>\n",
       "      <th>AGE_NMBR_COM</th>\n",
       "      <th>AGE_GRP_COM</th>\n",
       "      <th>SDC_COB_COM</th>\n",
       "      <th>SDC_COB_OTSP_COM</th>\n",
       "      <th>SDC_YACA_YR_COM</th>\n",
       "      <th>SDC_ETHN_CA_COM</th>\n",
       "      <th>...</th>\n",
       "      <th>ADM_EPIGEN2_COM</th>\n",
       "      <th>DNAmAge_COM</th>\n",
       "      <th>AgeAccelerationDifference_COM</th>\n",
       "      <th>AgeAccelerationResidual_COM</th>\n",
       "      <th>IEAA_COM</th>\n",
       "      <th>EEAA_COM</th>\n",
       "      <th>Hannum_Age_COM</th>\n",
       "      <th>SDC_POPCNTR_COM</th>\n",
       "      <th>SDC_POPDENSITY_COM</th>\n",
       "      <th>ADM_COMPLETE_MCQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77276078</td>\n",
       "      <td>fr</td>\n",
       "      <td>2013-07-23T08:09:06.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>8611.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89205938</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-09-28T17:11:01.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>77</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2873.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45509315</td>\n",
       "      <td>en</td>\n",
       "      <td>2015-03-18T11:43:11.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2491.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39135262</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-05-07T18:01:57.000-04</td>\n",
       "      <td>F</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1438.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70296812</td>\n",
       "      <td>fr</td>\n",
       "      <td>2012-12-05T14:49:54.000-05</td>\n",
       "      <td>M</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3551.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>91983272</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-04-03T15:04:31.000-04</td>\n",
       "      <td>F</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3355.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>92823730</td>\n",
       "      <td>fr</td>\n",
       "      <td>2014-10-21T09:53:31.000-04</td>\n",
       "      <td>F</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>9.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71376023</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-06-24T18:05:18.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>429.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25056251</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-05-28T15:13:11.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1165.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85698612</td>\n",
       "      <td>en</td>\n",
       "      <td>2015-02-18T11:29:36.000-05</td>\n",
       "      <td>F</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1651.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 3840 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity_id startlanguage_COM               startdate_COM SEX_ASK_COM  \\\n",
       "0   77276078                fr  2013-07-23T08:09:06.000-04           M   \n",
       "1   89205938                en  2013-09-28T17:11:01.000-04           M   \n",
       "2   45509315                en  2015-03-18T11:43:11.000-04           M   \n",
       "3   39135262                en  2013-05-07T18:01:57.000-04           F   \n",
       "4   70296812                fr  2012-12-05T14:49:54.000-05           M   \n",
       "5   91983272                en  2013-04-03T15:04:31.000-04           F   \n",
       "6   92823730                fr  2014-10-21T09:53:31.000-04           F   \n",
       "7   71376023                en  2013-06-24T18:05:18.000-04           M   \n",
       "8   25056251                en  2013-05-28T15:13:11.000-04           M   \n",
       "9   85698612                en  2015-02-18T11:29:36.000-05           F   \n",
       "\n",
       "   AGE_NMBR_COM  AGE_GRP_COM  SDC_COB_COM SDC_COB_OTSP_COM  SDC_YACA_YR_COM  \\\n",
       "0            50            1            1              NaN              NaN   \n",
       "1            77            4            1              NaN              NaN   \n",
       "2            63            2            1              NaN              NaN   \n",
       "3            59            2            1              NaN              NaN   \n",
       "4            67            3            1              NaN              NaN   \n",
       "5            56            2            1              NaN              NaN   \n",
       "6            76            4            1              NaN              NaN   \n",
       "7            66            3            1              NaN              NaN   \n",
       "8            70            3            1              NaN              NaN   \n",
       "9            67            3            1              NaN              NaN   \n",
       "\n",
       "   SDC_ETHN_CA_COM  ...  ADM_EPIGEN2_COM  DNAmAge_COM  \\\n",
       "0                1  ...              NaN          NaN   \n",
       "1                1  ...              NaN          NaN   \n",
       "2                1  ...              NaN          NaN   \n",
       "3                0  ...              NaN          NaN   \n",
       "4                1  ...              NaN          NaN   \n",
       "5                0  ...              NaN          NaN   \n",
       "6                1  ...              NaN          NaN   \n",
       "7                0  ...              NaN          NaN   \n",
       "8                0  ...              NaN          NaN   \n",
       "9                1  ...              NaN          NaN   \n",
       "\n",
       "   AgeAccelerationDifference_COM  AgeAccelerationResidual_COM  IEAA_COM  \\\n",
       "0                            NaN                          NaN       NaN   \n",
       "1                            NaN                          NaN       NaN   \n",
       "2                            NaN                          NaN       NaN   \n",
       "3                            NaN                          NaN       NaN   \n",
       "4                            NaN                          NaN       NaN   \n",
       "5                            NaN                          NaN       NaN   \n",
       "6                            NaN                          NaN       NaN   \n",
       "7                            NaN                          NaN       NaN   \n",
       "8                            NaN                          NaN       NaN   \n",
       "9                            NaN                          NaN       NaN   \n",
       "\n",
       "   EEAA_COM  Hannum_Age_COM  SDC_POPCNTR_COM  SDC_POPDENSITY_COM  \\\n",
       "0       NaN             NaN                4             8611.11   \n",
       "1       NaN             NaN                4             2873.91   \n",
       "2       NaN             NaN                4             2491.43   \n",
       "3       NaN             NaN                4             1438.46   \n",
       "4       NaN             NaN                4             3551.85   \n",
       "5       NaN             NaN                4             3355.26   \n",
       "6       NaN             NaN                1                9.68   \n",
       "7       NaN             NaN                1              429.20   \n",
       "8       NaN             NaN                3             1165.45   \n",
       "9       NaN             NaN                4             1651.28   \n",
       "\n",
       "   ADM_COMPLETE_MCQ  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  \n",
       "5                 1  \n",
       "6                 1  \n",
       "7                 1  \n",
       "8                 1  \n",
       "9                 1  \n",
       "\n",
       "[10 rows x 3840 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered DataFrame has 26622 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>startlanguage_COM</th>\n",
       "      <th>startdate_COM</th>\n",
       "      <th>SEX_ASK_COM</th>\n",
       "      <th>AGE_NMBR_COM</th>\n",
       "      <th>AGE_GRP_COM</th>\n",
       "      <th>SDC_COB_COM</th>\n",
       "      <th>SDC_COB_OTSP_COM</th>\n",
       "      <th>SDC_YACA_YR_COM</th>\n",
       "      <th>SDC_ETHN_CA_COM</th>\n",
       "      <th>...</th>\n",
       "      <th>ADM_EPIGEN2_COM</th>\n",
       "      <th>DNAmAge_COM</th>\n",
       "      <th>AgeAccelerationDifference_COM</th>\n",
       "      <th>AgeAccelerationResidual_COM</th>\n",
       "      <th>IEAA_COM</th>\n",
       "      <th>EEAA_COM</th>\n",
       "      <th>Hannum_Age_COM</th>\n",
       "      <th>SDC_POPCNTR_COM</th>\n",
       "      <th>SDC_POPDENSITY_COM</th>\n",
       "      <th>ADM_COMPLETE_MCQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77276078</td>\n",
       "      <td>fr</td>\n",
       "      <td>2013-07-23T08:09:06.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>8611.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89205938</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-09-28T17:11:01.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>77</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2873.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45509315</td>\n",
       "      <td>en</td>\n",
       "      <td>2015-03-18T11:43:11.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2491.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39135262</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-05-07T18:01:57.000-04</td>\n",
       "      <td>F</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1438.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70296812</td>\n",
       "      <td>fr</td>\n",
       "      <td>2012-12-05T14:49:54.000-05</td>\n",
       "      <td>M</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3551.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>91983272</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-04-03T15:04:31.000-04</td>\n",
       "      <td>F</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3355.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>92823730</td>\n",
       "      <td>fr</td>\n",
       "      <td>2014-10-21T09:53:31.000-04</td>\n",
       "      <td>F</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>9.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71376023</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-06-24T18:05:18.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>429.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85698612</td>\n",
       "      <td>en</td>\n",
       "      <td>2015-02-18T11:29:36.000-05</td>\n",
       "      <td>F</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1651.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>92184146</td>\n",
       "      <td>fr</td>\n",
       "      <td>2014-03-06T19:00:25.000-05</td>\n",
       "      <td>F</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2231.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 3840 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity_id startlanguage_COM               startdate_COM SEX_ASK_COM  \\\n",
       "0    77276078                fr  2013-07-23T08:09:06.000-04           M   \n",
       "1    89205938                en  2013-09-28T17:11:01.000-04           M   \n",
       "2    45509315                en  2015-03-18T11:43:11.000-04           M   \n",
       "3    39135262                en  2013-05-07T18:01:57.000-04           F   \n",
       "4    70296812                fr  2012-12-05T14:49:54.000-05           M   \n",
       "5    91983272                en  2013-04-03T15:04:31.000-04           F   \n",
       "6    92823730                fr  2014-10-21T09:53:31.000-04           F   \n",
       "7    71376023                en  2013-06-24T18:05:18.000-04           M   \n",
       "9    85698612                en  2015-02-18T11:29:36.000-05           F   \n",
       "10   92184146                fr  2014-03-06T19:00:25.000-05           F   \n",
       "\n",
       "    AGE_NMBR_COM  AGE_GRP_COM  SDC_COB_COM SDC_COB_OTSP_COM  SDC_YACA_YR_COM  \\\n",
       "0             50            1            1              NaN              NaN   \n",
       "1             77            4            1              NaN              NaN   \n",
       "2             63            2            1              NaN              NaN   \n",
       "3             59            2            1              NaN              NaN   \n",
       "4             67            3            1              NaN              NaN   \n",
       "5             56            2            1              NaN              NaN   \n",
       "6             76            4            1              NaN              NaN   \n",
       "7             66            3            1              NaN              NaN   \n",
       "9             67            3            1              NaN              NaN   \n",
       "10            49            1            1              NaN              NaN   \n",
       "\n",
       "    SDC_ETHN_CA_COM  ...  ADM_EPIGEN2_COM  DNAmAge_COM  \\\n",
       "0                 1  ...              NaN          NaN   \n",
       "1                 1  ...              NaN          NaN   \n",
       "2                 1  ...              NaN          NaN   \n",
       "3                 0  ...              NaN          NaN   \n",
       "4                 1  ...              NaN          NaN   \n",
       "5                 0  ...              NaN          NaN   \n",
       "6                 1  ...              NaN          NaN   \n",
       "7                 0  ...              NaN          NaN   \n",
       "9                 1  ...              NaN          NaN   \n",
       "10                1  ...              NaN          NaN   \n",
       "\n",
       "    AgeAccelerationDifference_COM  AgeAccelerationResidual_COM  IEAA_COM  \\\n",
       "0                             NaN                          NaN       NaN   \n",
       "1                             NaN                          NaN       NaN   \n",
       "2                             NaN                          NaN       NaN   \n",
       "3                             NaN                          NaN       NaN   \n",
       "4                             NaN                          NaN       NaN   \n",
       "5                             NaN                          NaN       NaN   \n",
       "6                             NaN                          NaN       NaN   \n",
       "7                             NaN                          NaN       NaN   \n",
       "9                             NaN                          NaN       NaN   \n",
       "10                            NaN                          NaN       NaN   \n",
       "\n",
       "    EEAA_COM  Hannum_Age_COM  SDC_POPCNTR_COM  SDC_POPDENSITY_COM  \\\n",
       "0        NaN             NaN                4             8611.11   \n",
       "1        NaN             NaN                4             2873.91   \n",
       "2        NaN             NaN                4             2491.43   \n",
       "3        NaN             NaN                4             1438.46   \n",
       "4        NaN             NaN                4             3551.85   \n",
       "5        NaN             NaN                4             3355.26   \n",
       "6        NaN             NaN                1                9.68   \n",
       "7        NaN             NaN                1              429.20   \n",
       "9        NaN             NaN                4             1651.28   \n",
       "10       NaN             NaN                2             2231.58   \n",
       "\n",
       "    ADM_COMPLETE_MCQ  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  \n",
       "5                  1  \n",
       "6                  1  \n",
       "7                  1  \n",
       "9                  1  \n",
       "10                 1  \n",
       "\n",
       "[10 rows x 3840 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['ADM_GWAS3_COM'])\n",
    "\n",
    "# Remove the decimal part and the trailing '.0' from the ADM_GWAS3_COM column\n",
    "df['ADM_GWAS3_COM'] = df['ADM_GWAS3_COM'].apply(lambda x: str(int(x)) if pd.notna(x) else '')\n",
    "\n",
    "print(f'The filtered DataFrame has {df.shape[0]} rows.')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered DataFrame has 24505 rows.\n",
      "24505\n"
     ]
    }
   ],
   "source": [
    "# Read the list of IDs of interest from the text file\n",
    "with open(\"EUR_ID_Regenie.txt\", \"r\") as f:\n",
    "    id_list = f.read().splitlines()\n",
    "\n",
    "#subset Europeans\n",
    "df = df[df[\"ADM_GWAS3_COM\"].isin(id_list)]\n",
    "print(f'The filtered DataFrame has {df.shape[0]} rows.')\n",
    "print(df[\"ADM_GWAS3_COM\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datatype of the 'ADM_GWAS3_COM' column in df to int64\n",
    "df['ADM_GWAS3_COM'] = df['ADM_GWAS3_COM'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PHENOTYPE\n",
    "phe_temp_DIA = pd.DataFrame({\n",
    "    \"FID\": df[\"ADM_GWAS3_COM\"].values,\n",
    "    \"IID\": df[\"ADM_GWAS3_COM\"].values,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id_df</th>\n",
       "      <th>startlanguage_COM_df</th>\n",
       "      <th>startdate_COM_df</th>\n",
       "      <th>SEX_ASK_COM_df</th>\n",
       "      <th>AGE_NMBR_COM_df</th>\n",
       "      <th>AGE_GRP_COM_df</th>\n",
       "      <th>SDC_COB_COM_df</th>\n",
       "      <th>SDC_COB_OTSP_COM_df</th>\n",
       "      <th>SDC_YACA_YR_COM_df</th>\n",
       "      <th>SDC_ETHN_CA_COM_df</th>\n",
       "      <th>...</th>\n",
       "      <th>ADM_EPIGEN2_COM_df</th>\n",
       "      <th>DNAmAge_COM_df</th>\n",
       "      <th>AgeAccelerationDifference_COM_df</th>\n",
       "      <th>AgeAccelerationResidual_COM_df</th>\n",
       "      <th>IEAA_COM_df</th>\n",
       "      <th>EEAA_COM_df</th>\n",
       "      <th>Hannum_Age_COM_df</th>\n",
       "      <th>SDC_POPCNTR_COM_df</th>\n",
       "      <th>SDC_POPDENSITY_COM_df</th>\n",
       "      <th>ADM_COMPLETE_MCQ_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77276078</td>\n",
       "      <td>fr</td>\n",
       "      <td>2013-07-23T08:09:06.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>8611.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89205938</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-09-28T17:11:01.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>77</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2873.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45509315</td>\n",
       "      <td>en</td>\n",
       "      <td>2015-03-18T11:43:11.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2491.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39135262</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-05-07T18:01:57.000-04</td>\n",
       "      <td>F</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1438.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70296812</td>\n",
       "      <td>fr</td>\n",
       "      <td>2012-12-05T14:49:54.000-05</td>\n",
       "      <td>M</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3551.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3840 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity_id_df startlanguage_COM_df            startdate_COM_df  \\\n",
       "0      77276078                   fr  2013-07-23T08:09:06.000-04   \n",
       "1      89205938                   en  2013-09-28T17:11:01.000-04   \n",
       "2      45509315                   en  2015-03-18T11:43:11.000-04   \n",
       "3      39135262                   en  2013-05-07T18:01:57.000-04   \n",
       "4      70296812                   fr  2012-12-05T14:49:54.000-05   \n",
       "\n",
       "  SEX_ASK_COM_df  AGE_NMBR_COM_df  AGE_GRP_COM_df  SDC_COB_COM_df  \\\n",
       "0              M               50               1               1   \n",
       "1              M               77               4               1   \n",
       "2              M               63               2               1   \n",
       "3              F               59               2               1   \n",
       "4              M               67               3               1   \n",
       "\n",
       "  SDC_COB_OTSP_COM_df  SDC_YACA_YR_COM_df  SDC_ETHN_CA_COM_df  ...  \\\n",
       "0                 NaN                 NaN                   1  ...   \n",
       "1                 NaN                 NaN                   1  ...   \n",
       "2                 NaN                 NaN                   1  ...   \n",
       "3                 NaN                 NaN                   0  ...   \n",
       "4                 NaN                 NaN                   1  ...   \n",
       "\n",
       "   ADM_EPIGEN2_COM_df  DNAmAge_COM_df  AgeAccelerationDifference_COM_df  \\\n",
       "0                 NaN             NaN                               NaN   \n",
       "1                 NaN             NaN                               NaN   \n",
       "2                 NaN             NaN                               NaN   \n",
       "3                 NaN             NaN                               NaN   \n",
       "4                 NaN             NaN                               NaN   \n",
       "\n",
       "   AgeAccelerationResidual_COM_df  IEAA_COM_df  EEAA_COM_df  \\\n",
       "0                             NaN          NaN          NaN   \n",
       "1                             NaN          NaN          NaN   \n",
       "2                             NaN          NaN          NaN   \n",
       "3                             NaN          NaN          NaN   \n",
       "4                             NaN          NaN          NaN   \n",
       "\n",
       "   Hannum_Age_COM_df  SDC_POPCNTR_COM_df  SDC_POPDENSITY_COM_df  \\\n",
       "0                NaN                   4                8611.11   \n",
       "1                NaN                   4                2873.91   \n",
       "2                NaN                   4                2491.43   \n",
       "3                NaN                   4                1438.46   \n",
       "4                NaN                   4                3551.85   \n",
       "\n",
       "   ADM_COMPLETE_MCQ_df  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    1  \n",
       "4                    1  \n",
       "\n",
       "[5 rows x 3840 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store the new column names to avoid error in the code when merging two datasets\n",
    "new_column_names = {}\n",
    "\n",
    "# Iterate over the existing column names\n",
    "for column in df.columns:\n",
    "    if column != 'ADM_GWAS3_COM':\n",
    "        new_column_names[column] = f\"{column}_df\"\n",
    "\n",
    "# Rename the columns using the dictionary\n",
    "df = df.rename(columns=new_column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>IID</th>\n",
       "      <th>entity_id_df</th>\n",
       "      <th>startlanguage_COM_df</th>\n",
       "      <th>startdate_COM_df</th>\n",
       "      <th>SEX_ASK_COM_df</th>\n",
       "      <th>AGE_NMBR_COM_df</th>\n",
       "      <th>AGE_GRP_COM_df</th>\n",
       "      <th>SDC_COB_COM_df</th>\n",
       "      <th>SDC_COB_OTSP_COM_df</th>\n",
       "      <th>...</th>\n",
       "      <th>ADM_EPIGEN2_COM_df</th>\n",
       "      <th>DNAmAge_COM_df</th>\n",
       "      <th>AgeAccelerationDifference_COM_df</th>\n",
       "      <th>AgeAccelerationResidual_COM_df</th>\n",
       "      <th>IEAA_COM_df</th>\n",
       "      <th>EEAA_COM_df</th>\n",
       "      <th>Hannum_Age_COM_df</th>\n",
       "      <th>SDC_POPCNTR_COM_df</th>\n",
       "      <th>SDC_POPDENSITY_COM_df</th>\n",
       "      <th>ADM_COMPLETE_MCQ_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17358</td>\n",
       "      <td>17358</td>\n",
       "      <td>77276078</td>\n",
       "      <td>fr</td>\n",
       "      <td>2013-07-23T08:09:06.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>8611.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19115</td>\n",
       "      <td>19115</td>\n",
       "      <td>89205938</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-09-28T17:11:01.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>77</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2873.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23576</td>\n",
       "      <td>23576</td>\n",
       "      <td>45509315</td>\n",
       "      <td>en</td>\n",
       "      <td>2015-03-18T11:43:11.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2491.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10778</td>\n",
       "      <td>10778</td>\n",
       "      <td>39135262</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-05-07T18:01:57.000-04</td>\n",
       "      <td>F</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1438.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6013</td>\n",
       "      <td>6013</td>\n",
       "      <td>70296812</td>\n",
       "      <td>fr</td>\n",
       "      <td>2012-12-05T14:49:54.000-05</td>\n",
       "      <td>M</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3551.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3842 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FID    IID  entity_id_df startlanguage_COM_df  \\\n",
       "0  17358  17358      77276078                   fr   \n",
       "1  19115  19115      89205938                   en   \n",
       "2  23576  23576      45509315                   en   \n",
       "3  10778  10778      39135262                   en   \n",
       "4   6013   6013      70296812                   fr   \n",
       "\n",
       "             startdate_COM_df SEX_ASK_COM_df  AGE_NMBR_COM_df  AGE_GRP_COM_df  \\\n",
       "0  2013-07-23T08:09:06.000-04              M               50               1   \n",
       "1  2013-09-28T17:11:01.000-04              M               77               4   \n",
       "2  2015-03-18T11:43:11.000-04              M               63               2   \n",
       "3  2013-05-07T18:01:57.000-04              F               59               2   \n",
       "4  2012-12-05T14:49:54.000-05              M               67               3   \n",
       "\n",
       "   SDC_COB_COM_df SDC_COB_OTSP_COM_df  ...  ADM_EPIGEN2_COM_df  \\\n",
       "0               1                 NaN  ...                 NaN   \n",
       "1               1                 NaN  ...                 NaN   \n",
       "2               1                 NaN  ...                 NaN   \n",
       "3               1                 NaN  ...                 NaN   \n",
       "4               1                 NaN  ...                 NaN   \n",
       "\n",
       "   DNAmAge_COM_df  AgeAccelerationDifference_COM_df  \\\n",
       "0             NaN                               NaN   \n",
       "1             NaN                               NaN   \n",
       "2             NaN                               NaN   \n",
       "3             NaN                               NaN   \n",
       "4             NaN                               NaN   \n",
       "\n",
       "   AgeAccelerationResidual_COM_df  IEAA_COM_df  EEAA_COM_df  \\\n",
       "0                             NaN          NaN          NaN   \n",
       "1                             NaN          NaN          NaN   \n",
       "2                             NaN          NaN          NaN   \n",
       "3                             NaN          NaN          NaN   \n",
       "4                             NaN          NaN          NaN   \n",
       "\n",
       "   Hannum_Age_COM_df  SDC_POPCNTR_COM_df  SDC_POPDENSITY_COM_df  \\\n",
       "0                NaN                   4                8611.11   \n",
       "1                NaN                   4                2873.91   \n",
       "2                NaN                   4                2491.43   \n",
       "3                NaN                   4                1438.46   \n",
       "4                NaN                   4                3551.85   \n",
       "\n",
       "   ADM_COMPLETE_MCQ_df  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    1  \n",
       "4                    1  \n",
       "\n",
       "[5 rows x 3842 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the two DataFrames on FID and ADM_GWAS3_COM\n",
    "merged = pd.merge(phe_temp_DIA, df, left_on='FID', right_on='ADM_GWAS3_COM', suffixes=('_phe', '_df'))\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n",
      "/tmp/ipykernel_2282412/2359952372.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[column] = pd.NaT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>IID</th>\n",
       "      <th>entity_id_df</th>\n",
       "      <th>startlanguage_COM_df</th>\n",
       "      <th>startdate_COM_df</th>\n",
       "      <th>SEX_ASK_COM_df</th>\n",
       "      <th>AGE_NMBR_COM_df</th>\n",
       "      <th>AGE_GRP_COM_df</th>\n",
       "      <th>SDC_COB_COM_df</th>\n",
       "      <th>SDC_COB_OTSP_COM_df</th>\n",
       "      <th>...</th>\n",
       "      <th>PKD_FACE_COM</th>\n",
       "      <th>PKD_RISE_COM</th>\n",
       "      <th>OST_MOM_COM</th>\n",
       "      <th>OST_CST_COM</th>\n",
       "      <th>OST_BP_COM</th>\n",
       "      <th>OST_BCKPPM_COM</th>\n",
       "      <th>ICQ_FXNOSE_COM</th>\n",
       "      <th>ICQ_FXCOLLR_COM</th>\n",
       "      <th>ICQ_SMOKE24H_COM</th>\n",
       "      <th>VA_GLASSES_COM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17358</td>\n",
       "      <td>17358</td>\n",
       "      <td>77276078</td>\n",
       "      <td>fr</td>\n",
       "      <td>2013-07-23T08:09:06.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19115</td>\n",
       "      <td>19115</td>\n",
       "      <td>89205938</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-09-28T17:11:01.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>77</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23576</td>\n",
       "      <td>23576</td>\n",
       "      <td>45509315</td>\n",
       "      <td>en</td>\n",
       "      <td>2015-03-18T11:43:11.000-04</td>\n",
       "      <td>M</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10778</td>\n",
       "      <td>10778</td>\n",
       "      <td>39135262</td>\n",
       "      <td>en</td>\n",
       "      <td>2013-05-07T18:01:57.000-04</td>\n",
       "      <td>F</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6013</td>\n",
       "      <td>6013</td>\n",
       "      <td>70296812</td>\n",
       "      <td>fr</td>\n",
       "      <td>2012-12-05T14:49:54.000-05</td>\n",
       "      <td>M</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FID    IID  entity_id_df startlanguage_COM_df  \\\n",
       "0  17358  17358      77276078                   fr   \n",
       "1  19115  19115      89205938                   en   \n",
       "2  23576  23576      45509315                   en   \n",
       "3  10778  10778      39135262                   en   \n",
       "4   6013   6013      70296812                   fr   \n",
       "\n",
       "             startdate_COM_df SEX_ASK_COM_df  AGE_NMBR_COM_df  AGE_GRP_COM_df  \\\n",
       "0  2013-07-23T08:09:06.000-04              M               50               1   \n",
       "1  2013-09-28T17:11:01.000-04              M               77               4   \n",
       "2  2015-03-18T11:43:11.000-04              M               63               2   \n",
       "3  2013-05-07T18:01:57.000-04              F               59               2   \n",
       "4  2012-12-05T14:49:54.000-05              M               67               3   \n",
       "\n",
       "   SDC_COB_COM_df SDC_COB_OTSP_COM_df  ...  PKD_FACE_COM  PKD_RISE_COM  \\\n",
       "0               1                 NaN  ...           NaT           NaT   \n",
       "1               1                 NaN  ...           NaT           NaT   \n",
       "2               1                 NaN  ...           NaT           NaT   \n",
       "3               1                 NaN  ...           NaT           NaT   \n",
       "4               1                 NaN  ...           NaT           NaT   \n",
       "\n",
       "   OST_MOM_COM  OST_CST_COM  OST_BP_COM  OST_BCKPPM_COM  ICQ_FXNOSE_COM  \\\n",
       "0          NaT          NaT         NaT             NaT             NaT   \n",
       "1          NaT          NaT         NaT             NaT             NaT   \n",
       "2          NaT          NaT         NaT             NaT             NaT   \n",
       "3          NaT          NaT         NaT             NaT             NaT   \n",
       "4          NaT          NaT         NaT             NaT             NaT   \n",
       "\n",
       "   ICQ_FXCOLLR_COM  ICQ_SMOKE24H_COM  VA_GLASSES_COM  \n",
       "0              NaT               NaT             NaT  \n",
       "1              NaT               NaT             NaT  \n",
       "2              NaT               NaT             NaT  \n",
       "3              NaT               NaT             NaT  \n",
       "4              NaT               NaT             NaT  \n",
       "\n",
       "[5 rows x 4232 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating empty columns for high-NA columns to be filled with proper values based on _df columns\n",
    "\n",
    "for column in lines:\n",
    "    merged[column] = pd.NaT\n",
    "    \n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 23087\n",
      "Number of missing values after re-encoding: 2\n"
     ]
    }
   ],
   "source": [
    "# HRG_USE_AID_COM\n",
    "merged.loc[merged['HRG_USE_AID_COM_df'] == 1, \"HRG_USE_AID_COM\"] = 1\n",
    "merged.loc[(merged[\"HRG_USE_AID_COM_df\"] == 0) | (merged[\"HRG_AID_COM_df\"] == 2), \"HRG_USE_AID_COM\"] = 0\n",
    "merged.loc[merged[\"HRG_USE_AID_COM_df\"].isin([pd.NaT]), \"HRG_USE_AID_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"HRG_USE_AID_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"HRG_USE_AID_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 22421\n",
      "Number of missing values after re-encoding: 5\n"
     ]
    }
   ],
   "source": [
    "# VIS_USE_MG_COM\n",
    "merged.loc[merged['VIS_USE_MG_COM_df'] == 1, \"VIS_USE_MG_COM\"] = 1\n",
    "merged.loc[(merged[\"VIS_USE_MG_COM_df\"] == 0) | (merged[\"VIS_AID_COM_df\"] == 2), \"VIS_USE_MG_COM\"] = 0\n",
    "merged.loc[merged[\"VIS_USE_MG_COM_df\"].isin([pd.NaT]), \"VIS_USE_MG_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"VIS_USE_MG_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"VIS_USE_MG_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 22281\n",
      "Number of missing values after re-encoding: 3\n"
     ]
    }
   ],
   "source": [
    "# SMK_TYPEOT_PI_COM\n",
    "merged.loc[merged['SMK_TYPEOT_PI_COM_df'] == 1, \"SMK_TYPEOT_PI_COM\"] = 1\n",
    "merged.loc[(merged[\"SMK_TYPEOT_PI_COM_df\"] == 0) | (merged[\"SMK_OTREG_COM_df\"] == 2), \"SMK_TYPEOT_PI_COM\"] = 0\n",
    "merged.loc[merged[\"SMK_TYPEOT_PI_COM_df\"].isin([pd.NaT]), \"SMK_TYPEOT_PI_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SMK_TYPEOT_PI_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SMK_TYPEOT_PI_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 22031\n",
      "Number of missing values after re-encoding: 3179\n"
     ]
    }
   ],
   "source": [
    "# TRA_MEDTPC_CON_MCQ\n",
    "merged.loc[merged['TRA_MEDTPC_CON_MCQ_df'] == 1, \"TRA_MEDTPC_CON_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_MEDTPC_CON_MCQ_df\"] == 0) | (merged[\"TRA_MED_MCQ_df\"] == 2), \"TRA_MEDTPC_CON_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_MEDTPC_CON_MCQ_df\"].isin([pd.NaT]), \"TRA_MEDTPC_CON_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_MEDTPC_CON_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_MEDTPC_CON_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 22031\n",
      "Number of missing values after re-encoding: 3179\n"
     ]
    }
   ],
   "source": [
    "# TRA_MEDTPC_ADV_MCQ\n",
    "merged.loc[merged['TRA_MEDTPC_ADV_MCQ_df'] == 1, \"TRA_MEDTPC_ADV_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_MEDTPC_ADV_MCQ_df\"] == 0) | (merged[\"TRA_MED_MCQ_df\"] == 2), \"TRA_MEDTPC_ADV_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_MEDTPC_ADV_MCQ_df\"].isin([pd.NaT]), \"TRA_MEDTPC_ADV_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_MEDTPC_ADV_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_MEDTPC_ADV_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 21227\n",
      "Number of missing values after re-encoding: 955\n"
     ]
    }
   ],
   "source": [
    "# PA2_MSPRT_GOL_MCQ\n",
    "merged.loc[merged['PA2_MSPRT_GOL_MCQ_df'] == 1, \"PA2_MSPRT_GOL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_MSPRT_GOL_MCQ_df\"] == 0) | (merged[\"PA2_MSPRT_MCQ_df\"] == 1), \"PA2_MSPRT_GOL_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_MSPRT_GOL_MCQ_df\"].isin([pd.NaT]), \"PA2_MSPRT_GOL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_MSPRT_GOL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_MSPRT_GOL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 21227\n",
      "Number of missing values after re-encoding: 955\n"
     ]
    }
   ],
   "source": [
    "# PA2_MSPRT_OT_MCQ\n",
    "merged.loc[merged['PA2_MSPRT_OT_MCQ_df'] == 1, \"PA2_MSPRT_OT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_MSPRT_OT_MCQ_df\"] == 0) | (merged[\"PA2_MSPRT_MCQ_df\"] == 1), \"PA2_MSPRT_OT_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_MSPRT_OT_MCQ_df\"].isin([pd.NaT]), \"PA2_MSPRT_OT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_MSPRT_OT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_MSPRT_OT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 20903\n",
      "Number of missing values after re-encoding: 23\n"
     ]
    }
   ],
   "source": [
    "# INJ_CAUS_FL_COM\n",
    "merged.loc[merged['INJ_CAUS_FL_COM_df'] == 1, \"INJ_CAUS_FL_COM\"] = 1\n",
    "merged.loc[(merged[\"INJ_CAUS_FL_COM_df\"] == 0) | (merged[\"INJ_OCC_COM_df\"] == 2), \"INJ_CAUS_FL_COM\"] = 0\n",
    "merged.loc[merged[\"INJ_CAUS_FL_COM_df\"].isin([pd.NaT]), \"INJ_CAUS_FL_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"INJ_CAUS_FL_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"INJ_CAUS_FL_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 20903\n",
      "Number of missing values after re-encoding: 23\n"
     ]
    }
   ],
   "source": [
    "# INJ_CAUS_NONE_COM\n",
    "merged.loc[merged['INJ_CAUS_NONE_COM_df'] == 1, \"INJ_CAUS_NONE_COM\"] = 1\n",
    "merged.loc[(merged[\"INJ_CAUS_NONE_COM_df\"] == 0) | (merged[\"INJ_OCC_COM_df\"] == 2), \"INJ_CAUS_NONE_COM\"] = 0\n",
    "merged.loc[merged[\"INJ_CAUS_NONE_COM_df\"].isin([pd.NaT]), \"INJ_CAUS_NONE_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"INJ_CAUS_NONE_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"INJ_CAUS_NONE_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 20797\n",
      "Number of missing values after re-encoding: 88\n"
     ]
    }
   ],
   "source": [
    "# OST_FRAC_OT_COM\n",
    "merged.loc[merged['OST_FRAC_OT_COM_df'] == 1, \"OST_FRAC_OT_COM\"] = 1\n",
    "merged.loc[(merged[\"OST_FRAC_OT_COM_df\"] == 0) | (merged[\"OST_BONE_COM_df\"] == 2), \"OST_FRAC_OT_COM\"] = 0\n",
    "merged.loc[merged[\"OST_FRAC_OT_COM_df\"].isin([pd.NaT]), \"OST_FRAC_OT_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"OST_FRAC_OT_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"OST_FRAC_OT_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 20791\n",
      "Number of missing values after re-encoding: 63\n"
     ]
    }
   ],
   "source": [
    "# CCC_CANTP_SNM_COM\n",
    "merged.loc[merged['CCC_CANTP_SNM_COM_df'] == 1, \"CCC_CANTP_SNM_COM\"] = 1\n",
    "merged.loc[(merged[\"CCC_CANTP_SNM_COM_df\"] == 0) | (merged[\"CCC_CANC_COM_df\"] == 2), \"CCC_CANTP_SNM_COM\"] = 0\n",
    "merged.loc[merged[\"CCC_CANTP_SNM_COM_df\"].isin([pd.NaT]), \"CCC_CANTP_SNM_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"CCC_CANTP_SNM_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"CCC_CANTP_SNM_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 19297\n",
      "Number of missing values after re-encoding: 954\n"
     ]
    }
   ],
   "source": [
    "# PA2_LSPRT_YOG_MCQ\n",
    "merged.loc[merged['PA2_LSPRT_YOG_MCQ_df'] == 1, \"PA2_LSPRT_YOG_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_LSPRT_YOG_MCQ_df\"] == 0) | (merged[\"PA2_LSPRT_MCQ_df\"] == 1), \"PA2_LSPRT_YOG_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_LSPRT_YOG_MCQ_df\"].isin([pd.NaT]), \"PA2_LSPRT_YOG_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_LSPRT_YOG_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_LSPRT_YOG_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 18783\n",
      "Number of missing values after re-encoding: 1550\n"
     ]
    }
   ],
   "source": [
    "# ROS_LOC_04_COM\n",
    "merged.loc[merged['ROS_LOC_04_COM_df'] == 1, \"ROS_LOC_04_COM\"] = 1\n",
    "merged.loc[(merged[\"ROS_LOC_04_COM_df\"] == 0) | (merged[\"ROS_PAIN_COM_df\"] == 2), \"ROS_LOC_04_COM\"] = 0\n",
    "merged.loc[merged[\"ROS_LOC_04_COM_df\"].isin([pd.NaT]), \"ROS_LOC_04_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ROS_LOC_04_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ROS_LOC_04_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 18783\n",
      "Number of missing values after re-encoding: 1550\n"
     ]
    }
   ],
   "source": [
    "# ROS_LOC_05_COM\n",
    "merged.loc[merged['ROS_LOC_05_COM_df'] == 1, \"ROS_LOC_05_COM\"] = 1\n",
    "merged.loc[(merged[\"ROS_LOC_05_COM_df\"] == 0) | (merged[\"ROS_PAIN_COM_df\"] == 2), \"ROS_LOC_05_COM\"] = 0\n",
    "merged.loc[merged[\"ROS_LOC_05_COM_df\"].isin([pd.NaT]), \"ROS_LOC_05_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ROS_LOC_05_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ROS_LOC_05_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 18783\n",
      "Number of missing values after re-encoding: 1550\n"
     ]
    }
   ],
   "source": [
    "# ROS_LOC_06_COM\n",
    "merged.loc[merged['ROS_LOC_06_COM_df'] == 1, \"ROS_LOC_06_COM\"] = 1\n",
    "merged.loc[(merged[\"ROS_LOC_06_COM_df\"] == 0) | (merged[\"ROS_PAIN_COM_df\"] == 2), \"ROS_LOC_06_COM\"] = 0\n",
    "merged.loc[merged[\"ROS_LOC_06_COM_df\"].isin([pd.NaT]), \"ROS_LOC_06_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ROS_LOC_06_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ROS_LOC_06_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 17669\n",
      "Number of missing values after re-encoding: 960\n"
     ]
    }
   ],
   "source": [
    "# PA2_EXER_CAL_MCQ\n",
    "merged.loc[merged['PA2_EXER_CAL_MCQ_df'] == 1, \"PA2_EXER_CAL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_EXER_CAL_MCQ_df\"] == 0) | (merged[\"PA2_EXER_MCQ_df\"] == 1), \"PA2_EXER_CAL_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_EXER_CAL_MCQ_df\"].isin([pd.NaT]), \"PA2_EXER_CAL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_EXER_CAL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_EXER_CAL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 17669\n",
      "Number of missing values after re-encoding: 960\n"
     ]
    }
   ],
   "source": [
    "# PA2_EXER_PUS_MCQ\n",
    "merged.loc[merged['PA2_EXER_PUS_MCQ_df'] == 1, \"PA2_EXER_PUS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_EXER_PUS_MCQ_df\"] == 0) | (merged[\"PA2_EXER_MCQ_df\"] == 1), \"PA2_EXER_PUS_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_EXER_PUS_MCQ_df\"].isin([pd.NaT]), \"PA2_EXER_PUS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_EXER_PUS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_EXER_PUS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 17669\n",
      "Number of missing values after re-encoding: 960\n"
     ]
    }
   ],
   "source": [
    "# PA2_EXER_SIT_MCQ\n",
    "merged.loc[merged['PA2_EXER_SIT_MCQ_df'] == 1, \"PA2_EXER_SIT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_EXER_SIT_MCQ_df\"] == 0) | (merged[\"PA2_EXER_MCQ_df\"] == 1), \"PA2_EXER_SIT_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_EXER_SIT_MCQ_df\"].isin([pd.NaT]), \"PA2_EXER_SIT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_EXER_SIT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_EXER_SIT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 17669\n",
      "Number of missing values after re-encoding: 960\n"
     ]
    }
   ],
   "source": [
    "# PA2_EXER_WEI_MCQ\n",
    "merged.loc[merged['PA2_EXER_WEI_MCQ_df'] == 1, \"PA2_EXER_WEI_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_EXER_WEI_MCQ_df\"] == 0) | (merged[\"PA2_EXER_MCQ_df\"] == 1), \"PA2_EXER_WEI_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_EXER_WEI_MCQ_df\"].isin([pd.NaT]), \"PA2_EXER_WEI_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_EXER_WEI_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_EXER_WEI_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 17669\n",
      "Number of missing values after re-encoding: 960\n"
     ]
    }
   ],
   "source": [
    "# PA2_EXER_OT_MCQ\n",
    "merged.loc[merged['PA2_EXER_OT_MCQ_df'] == 1, \"PA2_EXER_OT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_EXER_OT_MCQ_df\"] == 0) | (merged[\"PA2_EXER_MCQ_df\"] == 1), \"PA2_EXER_OT_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_EXER_OT_MCQ_df\"].isin([pd.NaT]), \"PA2_EXER_OT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_EXER_OT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_EXER_OT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16550\n",
      "Number of missing values after re-encoding: 32\n"
     ]
    }
   ],
   "source": [
    "# TBI_RSLT_DZ_COM\n",
    "merged.loc[merged['TBI_RSLT_DZ_COM_df'] == 1, \"TBI_RSLT_DZ_COM\"] = 1\n",
    "merged.loc[(merged[\"TBI_RSLT_DZ_COM_df\"] == 0) | ((merged[\"TBI_TYP_VH_COM_df\"] == 0) & (merged[\"TBI_TYP_FL_COM_df\"] == 0) & (merged[\"TBI_TYP_SPT_COM_df\"] == 0)), \"TBI_RSLT_DZ_COM\"] = 0\n",
    "merged.loc[merged[\"TBI_RSLT_DZ_COM_df\"].isin([pd.NaT]), \"TBI_RSLT_DZ_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TBI_RSLT_DZ_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TBI_RSLT_DZ_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16550\n",
      "Number of missing values after re-encoding: 32\n"
     ]
    }
   ],
   "source": [
    "# TBI_RSLT_DRM_COM\n",
    "merged.loc[merged['TBI_RSLT_DRM_COM_df'] == 1, \"TBI_RSLT_DRM_COM\"] = 1\n",
    "merged.loc[(merged[\"TBI_RSLT_DRM_COM_df\"] == 0) | ((merged[\"TBI_TYP_VH_COM_df\"] == 0) & (merged[\"TBI_TYP_FL_COM_df\"] == 0) & (merged[\"TBI_TYP_SPT_COM_df\"] == 0)), \"TBI_RSLT_DRM_COM\"] = 0\n",
    "merged.loc[merged[\"TBI_RSLT_DRM_COM_df\"].isin([pd.NaT]), \"TBI_RSLT_DRM_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TBI_RSLT_DRM_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TBI_RSLT_DRM_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16550\n",
      "Number of missing values after re-encoding: 32\n"
     ]
    }
   ],
   "source": [
    "# TBI_RSLT_KO1_COM\n",
    "merged.loc[merged['TBI_RSLT_KO1_COM_df'] == 1, \"TBI_RSLT_KO1_COM\"] = 1\n",
    "merged.loc[(merged[\"TBI_RSLT_KO1_COM_df\"] == 0) | ((merged[\"TBI_TYP_VH_COM_df\"] == 0) & (merged[\"TBI_TYP_FL_COM_df\"] == 0) & (merged[\"TBI_TYP_SPT_COM_df\"] == 0)), \"TBI_RSLT_KO1_COM\"] = 0\n",
    "merged.loc[merged[\"TBI_RSLT_KO1_COM_df\"].isin([pd.NaT]), \"TBI_RSLT_KO1_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TBI_RSLT_KO1_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TBI_RSLT_KO1_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16550\n",
      "Number of missing values after re-encoding: 32\n"
     ]
    }
   ],
   "source": [
    "# TBI_RSLT_KO20_COM\n",
    "merged.loc[merged['TBI_RSLT_KO20_COM_df'] == 1, \"TBI_RSLT_KO20_COM\"] = 1\n",
    "merged.loc[(merged[\"TBI_RSLT_KO20_COM_df\"] == 0) | ((merged[\"TBI_TYP_VH_COM_df\"] == 0) & (merged[\"TBI_TYP_FL_COM_df\"] == 0) & (merged[\"TBI_TYP_SPT_COM_df\"] == 0)), \"TBI_RSLT_KO20_COM\"] = 0\n",
    "merged.loc[merged[\"TBI_RSLT_KO20_COM_df\"].isin([pd.NaT]), \"TBI_RSLT_KO20_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TBI_RSLT_KO20_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TBI_RSLT_KO20_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16550\n",
      "Number of missing values after re-encoding: 32\n"
     ]
    }
   ],
   "source": [
    "# TBI_RSLT_NONE_COM\n",
    "merged.loc[merged['TBI_RSLT_NONE_COM_df'] == 1, \"TBI_RSLT_NONE_COM\"] = 1\n",
    "merged.loc[(merged[\"TBI_RSLT_NONE_COM_df\"] == 0) | ((merged[\"TBI_TYP_VH_COM_df\"] == 0) & (merged[\"TBI_TYP_FL_COM_df\"] == 0) & (merged[\"TBI_TYP_SPT_COM_df\"] == 0)), \"TBI_RSLT_NONE_COM\"] = 0\n",
    "merged.loc[merged[\"TBI_RSLT_NONE_COM_df\"].isin([pd.NaT]), \"TBI_RSLT_NONE_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TBI_RSLT_NONE_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TBI_RSLT_NONE_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16550\n",
      "Number of missing values after re-encoding: 32\n"
     ]
    }
   ],
   "source": [
    "# TBI_MCR_PHA_COM\n",
    "merged.loc[merged['TBI_MCR_PHA_COM_df'] == 1, \"TBI_MCR_PHA_COM\"] = 1\n",
    "merged.loc[(merged[\"TBI_MCR_PHA_COM_df\"] == 0) | ((merged[\"TBI_TYP_VH_COM_df\"] == 0) & (merged[\"TBI_TYP_FL_COM_df\"] == 0) & (merged[\"TBI_TYP_SPT_COM_df\"] == 0)), \"TBI_MCR_PHA_COM\"] = 0\n",
    "merged.loc[merged[\"TBI_MCR_PHA_COM_df\"].isin([pd.NaT]), \"TBI_MCR_PHA_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TBI_MCR_PHA_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TBI_MCR_PHA_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16550\n",
      "Number of missing values after re-encoding: 32\n"
     ]
    }
   ],
   "source": [
    "# TBI_MCR_ED_COM\n",
    "merged.loc[merged['TBI_MCR_ED_COM_df'] == 1, \"TBI_MCR_ED_COM\"] = 1\n",
    "merged.loc[(merged[\"TBI_MCR_ED_COM_df\"] == 0) | ((merged[\"TBI_TYP_VH_COM_df\"] == 0) & (merged[\"TBI_TYP_FL_COM_df\"] == 0) & (merged[\"TBI_TYP_SPT_COM_df\"] == 0)), \"TBI_MCR_ED_COM\"] = 0\n",
    "merged.loc[merged[\"TBI_MCR_ED_COM_df\"].isin([pd.NaT]), \"TBI_MCR_ED_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TBI_MCR_ED_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TBI_MCR_ED_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16550\n",
      "Number of missing values after re-encoding: 32\n"
     ]
    }
   ],
   "source": [
    "# TBI_MCR_HO_COM\n",
    "merged.loc[merged['TBI_MCR_HO_COM_df'] == 1, \"TBI_MCR_HO_COM\"] = 1\n",
    "merged.loc[(merged[\"TBI_MCR_HO_COM_df\"] == 0) | ((merged[\"TBI_TYP_VH_COM_df\"] == 0) & (merged[\"TBI_TYP_FL_COM_df\"] == 0) & (merged[\"TBI_TYP_SPT_COM_df\"] == 0)), \"TBI_MCR_HO_COM\"] = 0\n",
    "merged.loc[merged[\"TBI_MCR_HO_COM_df\"].isin([pd.NaT]), \"TBI_MCR_HO_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TBI_MCR_HO_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TBI_MCR_HO_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16550\n",
      "Number of missing values after re-encoding: 32\n"
     ]
    }
   ],
   "source": [
    "# TBI_MCR_NONE_COM\n",
    "merged.loc[merged['TBI_MCR_NONE_COM_df'] == 1, \"TBI_MCR_NONE_COM\"] = 1\n",
    "merged.loc[(merged[\"TBI_MCR_NONE_COM_df\"] == 0) | ((merged[\"TBI_TYP_VH_COM_df\"] == 0) & (merged[\"TBI_TYP_FL_COM_df\"] == 0) & (merged[\"TBI_TYP_SPT_COM_df\"] == 0)), \"TBI_MCR_NONE_COM\"] = 0\n",
    "merged.loc[merged[\"TBI_MCR_NONE_COM_df\"].isin([pd.NaT]), \"TBI_MCR_NONE_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TBI_MCR_NONE_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TBI_MCR_NONE_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16550\n",
      "Number of missing values after re-encoding: 32\n"
     ]
    }
   ],
   "source": [
    "# TBI_PROB_NONE_COM\n",
    "merged.loc[merged['TBI_PROB_NONE_COM_df'] == 1, \"TBI_PROB_NONE_COM\"] = 1\n",
    "merged.loc[(merged[\"TBI_PROB_NONE_COM_df\"] == 0) | ((merged[\"TBI_TYP_VH_COM_df\"] == 0) & (merged[\"TBI_TYP_FL_COM_df\"] == 0) & (merged[\"TBI_TYP_SPT_COM_df\"] == 0)), \"TBI_PROB_NONE_COM\"] = 0\n",
    "merged.loc[merged[\"TBI_PROB_NONE_COM_df\"].isin([pd.NaT]), \"TBI_PROB_NONE_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TBI_PROB_NONE_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TBI_PROB_NONE_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16476\n",
      "Number of missing values after re-encoding: 957\n"
     ]
    }
   ],
   "source": [
    "# PA2_SSPRT_AER_MCQ\n",
    "merged.loc[merged['PA2_SSPRT_AER_MCQ_df'] == 1, \"PA2_SSPRT_AER_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SSPRT_AER_MCQ_df\"] == 0) | (merged[\"PA2_SSPRT_MCQ_df\"] == 1), \"PA2_SSPRT_AER_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SSPRT_AER_MCQ_df\"].isin([pd.NaT]), \"PA2_SSPRT_AER_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SSPRT_AER_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SSPRT_AER_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16476\n",
      "Number of missing values after re-encoding: 957\n"
     ]
    }
   ],
   "source": [
    "# PA2_SSPRT_BIC_MCQ\n",
    "merged.loc[merged['PA2_SSPRT_BIC_MCQ_df'] == 1, \"PA2_SSPRT_BIC_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SSPRT_BIC_MCQ_df\"] == 0) | (merged[\"PA2_SSPRT_MCQ_df\"] == 1), \"PA2_SSPRT_BIC_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SSPRT_BIC_MCQ_df\"].isin([pd.NaT]), \"PA2_SSPRT_BIC_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SSPRT_BIC_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SSPRT_BIC_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16476\n",
      "Number of missing values after re-encoding: 957\n"
     ]
    }
   ],
   "source": [
    "# PA2_SSPRT_JOG_MCQ\n",
    "merged.loc[merged['PA2_SSPRT_JOG_MCQ_df'] == 1, \"PA2_SSPRT_JOG_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SSPRT_JOG_MCQ_df\"] == 0) | (merged[\"PA2_SSPRT_MCQ_df\"] == 1), \"PA2_SSPRT_JOG_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SSPRT_JOG_MCQ_df\"].isin([pd.NaT]), \"PA2_SSPRT_JOG_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SSPRT_JOG_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SSPRT_JOG_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16476\n",
      "Number of missing values after re-encoding: 957\n"
     ]
    }
   ],
   "source": [
    "# PA2_SSPRT_SWI_MCQ\n",
    "merged.loc[merged['PA2_SSPRT_SWI_MCQ_df'] == 1, \"PA2_SSPRT_SWI_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SSPRT_SWI_MCQ_df\"] == 0) | (merged[\"PA2_SSPRT_MCQ_df\"] == 1), \"PA2_SSPRT_SWI_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SSPRT_SWI_MCQ_df\"].isin([pd.NaT]), \"PA2_SSPRT_SWI_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SSPRT_SWI_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SSPRT_SWI_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16476\n",
      "Number of missing values after re-encoding: 957\n"
     ]
    }
   ],
   "source": [
    "# PA2_SSPRT_OT_MCQ\n",
    "merged.loc[merged['PA2_SSPRT_OT_MCQ_df'] == 1, \"PA2_SSPRT_OT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SSPRT_OT_MCQ_df\"] == 0) | (merged[\"PA2_SSPRT_MCQ_df\"] == 1), \"PA2_SSPRT_OT_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SSPRT_OT_MCQ_df\"].isin([pd.NaT]), \"PA2_SSPRT_OT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SSPRT_OT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SSPRT_OT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 12977\n",
      "Number of missing values after re-encoding: 53\n"
     ]
    }
   ],
   "source": [
    "# SPA_PREVAC_CO_COM\n",
    "merged.loc[merged['SPA_PREVAC_CO_COM_df'] == 1, \"SPA_PREVAC_CO_COM\"] = 1\n",
    "merged.loc[(merged[\"SPA_PREVAC_CO_COM_df\"] == 0) | (merged[\"SPA_MORAC_COM_df\"] == 2), \"SPA_PREVAC_CO_COM\"] = 0\n",
    "merged.loc[merged[\"SPA_PREVAC_CO_COM_df\"].isin([pd.NaT]), \"SPA_PREVAC_CO_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SPA_PREVAC_CO_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SPA_PREVAC_CO_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 12977\n",
      "Number of missing values after re-encoding: 53\n"
     ]
    }
   ],
   "source": [
    "# SPA_PREVAC_HC_COM\n",
    "merged.loc[merged['SPA_PREVAC_HC_COM_df'] == 1, \"SPA_PREVAC_HC_COM\"] = 1\n",
    "merged.loc[(merged[\"SPA_PREVAC_HC_COM_df\"] == 0) | (merged[\"SPA_MORAC_COM_df\"] == 2), \"SPA_PREVAC_HC_COM\"] = 0\n",
    "merged.loc[merged[\"SPA_PREVAC_HC_COM_df\"].isin([pd.NaT]), \"SPA_PREVAC_HC_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SPA_PREVAC_HC_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SPA_PREVAC_HC_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 12977\n",
      "Number of missing values after re-encoding: 53\n"
     ]
    }
   ],
   "source": [
    "# SPA_PREVAC_TI_COM\n",
    "merged.loc[merged['SPA_PREVAC_TI_COM_df'] == 1, \"SPA_PREVAC_TI_COM\"] = 1\n",
    "merged.loc[(merged[\"SPA_PREVAC_TI_COM_df\"] == 0) | (merged[\"SPA_MORAC_COM_df\"] == 2), \"SPA_PREVAC_TI_COM\"] = 0\n",
    "merged.loc[merged[\"SPA_PREVAC_TI_COM_df\"].isin([pd.NaT]), \"SPA_PREVAC_TI_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SPA_PREVAC_TI_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SPA_PREVAC_TI_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 12977\n",
      "Number of missing values after re-encoding: 53\n"
     ]
    }
   ],
   "source": [
    "# SPA_PREVAC_GA_COM\n",
    "merged.loc[merged['SPA_PREVAC_GA_COM_df'] == 1, \"SPA_PREVAC_GA_COM\"] = 1\n",
    "merged.loc[(merged[\"SPA_PREVAC_GA_COM_df\"] == 0) | (merged[\"SPA_MORAC_COM_df\"] == 2), \"SPA_PREVAC_GA_COM\"] = 0\n",
    "merged.loc[merged[\"SPA_PREVAC_GA_COM_df\"].isin([pd.NaT]), \"SPA_PREVAC_GA_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SPA_PREVAC_GA_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SPA_PREVAC_GA_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 12977\n",
      "Number of missing values after re-encoding: 53\n"
     ]
    }
   ],
   "source": [
    "# SPA_PREVAC_PR_COM\n",
    "merged.loc[merged['SPA_PREVAC_PR_COM_df'] == 1, \"SPA_PREVAC_PR_COM\"] = 1\n",
    "merged.loc[(merged[\"SPA_PREVAC_PR_COM_df\"] == 0) | (merged[\"SPA_MORAC_COM_df\"] == 2), \"SPA_PREVAC_PR_COM\"] = 0\n",
    "merged.loc[merged[\"SPA_PREVAC_PR_COM_df\"].isin([pd.NaT]), \"SPA_PREVAC_PR_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SPA_PREVAC_PR_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SPA_PREVAC_PR_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 12977\n",
      "Number of missing values after re-encoding: 53\n"
     ]
    }
   ],
   "source": [
    "# SPA_PREVAC_TB_COM\n",
    "merged.loc[merged['SPA_PREVAC_TB_COM_df'] == 1, \"SPA_PREVAC_TB_COM\"] = 1\n",
    "merged.loc[(merged[\"SPA_PREVAC_TB_COM_df\"] == 0) | (merged[\"SPA_MORAC_COM_df\"] == 2), \"SPA_PREVAC_TB_COM\"] = 0\n",
    "merged.loc[merged[\"SPA_PREVAC_TB_COM_df\"].isin([pd.NaT]), \"SPA_PREVAC_TB_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SPA_PREVAC_TB_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SPA_PREVAC_TB_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 12977\n",
      "Number of missing values after re-encoding: 53\n"
     ]
    }
   ],
   "source": [
    "# SPA_PREVAC_MO_COM\n",
    "merged.loc[merged['SPA_PREVAC_MO_COM_df'] == 1, \"SPA_PREVAC_MO_COM\"] = 1\n",
    "merged.loc[(merged[\"SPA_PREVAC_MO_COM_df\"] == 0) | (merged[\"SPA_MORAC_COM_df\"] == 2), \"SPA_PREVAC_MO_COM\"] = 0\n",
    "merged.loc[merged[\"SPA_PREVAC_MO_COM_df\"].isin([pd.NaT]), \"SPA_PREVAC_MO_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SPA_PREVAC_MO_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SPA_PREVAC_MO_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 12775\n",
      "Number of missing values after re-encoding: 2970\n"
     ]
    }
   ],
   "source": [
    "# INT_WYSSCL_FRI_MCQ\n",
    "merged.loc[merged['INT_WYSSCL_FRI_MCQ_df'] == 1, \"INT_WYSSCL_FRI_MCQ\"] = 1\n",
    "merged.loc[(merged[\"INT_WYSSCL_FRI_MCQ_df\"] == 0) | (merged[\"INT_SCLNTWRK_MCQ_df\"] == 2), \"INT_WYSSCL_FRI_MCQ\"] = 0\n",
    "merged.loc[merged[\"INT_WYSSCL_FRI_MCQ_df\"].isin([pd.NaT]), \"INT_WYSSCL_FRI_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"INT_WYSSCL_FRI_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"INT_WYSSCL_FRI_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 12775\n",
      "Number of missing values after re-encoding: 2970\n"
     ]
    }
   ],
   "source": [
    "# INT_WYSSCL_FAM_MCQ\n",
    "merged.loc[merged['INT_WYSSCL_FAM_MCQ_df'] == 1, \"INT_WYSSCL_FAM_MCQ\"] = 1\n",
    "merged.loc[(merged[\"INT_WYSSCL_FAM_MCQ_df\"] == 0) | (merged[\"INT_SCLNTWRK_MCQ_df\"] == 2), \"INT_WYSSCL_FAM_MCQ\"] = 0\n",
    "merged.loc[merged[\"INT_WYSSCL_FAM_MCQ_df\"].isin([pd.NaT]), \"INT_WYSSCL_FAM_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"INT_WYSSCL_FAM_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"INT_WYSSCL_FAM_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 12775\n",
      "Number of missing values after re-encoding: 2970\n"
     ]
    }
   ],
   "source": [
    "# INT_WYSSCL_PRO_MCQ\n",
    "merged.loc[merged['INT_WYSSCL_PRO_MCQ_df'] == 1, \"INT_WYSSCL_PRO_MCQ\"] = 1\n",
    "merged.loc[(merged[\"INT_WYSSCL_PRO_MCQ_df\"] == 0) | (merged[\"INT_SCLNTWRK_MCQ_df\"] == 2), \"INT_WYSSCL_PRO_MCQ\"] = 0\n",
    "merged.loc[merged[\"INT_WYSSCL_PRO_MCQ_df\"].isin([pd.NaT]), \"INT_WYSSCL_PRO_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"INT_WYSSCL_PRO_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"INT_WYSSCL_PRO_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10955\n",
      "Number of missing values after re-encoding: 1007\n"
     ]
    }
   ],
   "source": [
    "# PA2_PRVPA_HEA_MCQ\n",
    "merged.loc[merged['PA2_PRVPA_HEA_MCQ_df'] == 1, \"PA2_PRVPA_HEA_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_PRVPA_HEA_MCQ_df\"] == 0) | (merged[\"PA2_PARTPA_MCQ_df\"] == 2), \"PA2_PRVPA_HEA_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_PRVPA_HEA_MCQ_df\"].isin([pd.NaT]), \"PA2_PRVPA_HEA_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_PRVPA_HEA_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_PRVPA_HEA_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10955\n",
      "Number of missing values after re-encoding: 1007\n"
     ]
    }
   ],
   "source": [
    "# PA2_PRVPA_ILL_MCQ\n",
    "merged.loc[merged['PA2_PRVPA_ILL_MCQ_df'] == 1, \"PA2_PRVPA_ILL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_PRVPA_ILL_MCQ_df\"] == 0) | (merged[\"PA2_PARTPA_MCQ_df\"] == 2), \"PA2_PRVPA_ILL_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_PRVPA_ILL_MCQ_df\"].isin([pd.NaT]), \"PA2_PRVPA_ILL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_PRVPA_ILL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_PRVPA_ILL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10955\n",
      "Number of missing values after re-encoding: 1007\n"
     ]
    }
   ],
   "source": [
    "# PA2_PRVPA_TIM_MCQ\n",
    "merged.loc[merged['PA2_PRVPA_TIM_MCQ_df'] == 1, \"PA2_PRVPA_TIM_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_PRVPA_TIM_MCQ_df\"] == 0) | (merged[\"PA2_PARTPA_MCQ_df\"] == 2), \"PA2_PRVPA_TIM_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_PRVPA_TIM_MCQ_df\"].isin([pd.NaT]), \"PA2_PRVPA_TIM_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_PRVPA_TIM_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_PRVPA_TIM_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10955\n",
      "Number of missing values after re-encoding: 1007\n"
     ]
    }
   ],
   "source": [
    "# PA2_PRVPA_ENG_MCQ\n",
    "merged.loc[merged['PA2_PRVPA_ENG_MCQ_df'] == 1, \"PA2_PRVPA_ENG_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_PRVPA_ENG_MCQ_df\"] == 0) | (merged[\"PA2_PARTPA_MCQ_df\"] == 2), \"PA2_PRVPA_ENG_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_PRVPA_ENG_MCQ_df\"].isin([pd.NaT]), \"PA2_PRVPA_ENG_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_PRVPA_ENG_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_PRVPA_ENG_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10955\n",
      "Number of missing values after re-encoding: 1007\n"
     ]
    }
   ],
   "source": [
    "# PA2_PRVPA_MOT_MCQ\n",
    "merged.loc[merged['PA2_PRVPA_MOT_MCQ_df'] == 1, \"PA2_PRVPA_MOT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_PRVPA_MOT_MCQ_df\"] == 0) | (merged[\"PA2_PARTPA_MCQ_df\"] == 2), \"PA2_PRVPA_MOT_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_PRVPA_MOT_MCQ_df\"].isin([pd.NaT]), \"PA2_PRVPA_MOT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_PRVPA_MOT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_PRVPA_MOT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10955\n",
      "Number of missing values after re-encoding: 1007\n"
     ]
    }
   ],
   "source": [
    "# PA2_PRVPA_OT_MCQ\n",
    "merged.loc[merged['PA2_PRVPA_OT_MCQ_df'] == 1, \"PA2_PRVPA_OT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_PRVPA_OT_MCQ_df\"] == 0) | (merged[\"PA2_PARTPA_MCQ_df\"] == 2), \"PA2_PRVPA_OT_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_PRVPA_OT_MCQ_df\"].isin([pd.NaT]), \"PA2_PRVPA_OT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_PRVPA_OT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_PRVPA_OT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10346\n",
      "Number of missing values after re-encoding: 91\n"
     ]
    }
   ],
   "source": [
    "# RET_WHY_CM_COM\n",
    "merged.loc[merged['RET_WHY_CM_COM_df'] == 1, \"RET_WHY_CM_COM\"] = 1\n",
    "merged.loc[(merged[\"RET_WHY_CM_COM_df\"] == 0) | (merged[\"RET_RTRD_COM_df\"] == 3), \"RET_WHY_CM_COM\"] = 0\n",
    "merged.loc[merged[\"RET_WHY_CM_COM_df\"].isin([pd.NaT]), \"RET_WHY_CM_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"RET_WHY_CM_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"RET_WHY_CM_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10346\n",
      "Number of missing values after re-encoding: 91\n"
     ]
    }
   ],
   "source": [
    "# RET_WHY_RE_COM\n",
    "merged.loc[merged['RET_WHY_RE_COM_df'] == 1, \"RET_WHY_RE_COM\"] = 1\n",
    "merged.loc[(merged[\"RET_WHY_RE_COM_df\"] == 0) | (merged[\"RET_RTRD_COM_df\"] == 3), \"RET_WHY_RE_COM\"] = 0\n",
    "merged.loc[merged[\"RET_WHY_RE_COM_df\"].isin([pd.NaT]), \"RET_WHY_RE_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"RET_WHY_RE_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"RET_WHY_RE_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10346\n",
      "Number of missing values after re-encoding: 91\n"
     ]
    }
   ],
   "source": [
    "# RET_WHY_HL_COM\n",
    "merged.loc[merged['RET_WHY_HL_COM_df'] == 1, \"RET_WHY_HL_COM\"] = 1\n",
    "merged.loc[(merged[\"RET_WHY_HL_COM_df\"] == 0) | (merged[\"RET_RTRD_COM_df\"] == 3), \"RET_WHY_HL_COM\"] = 0\n",
    "merged.loc[merged[\"RET_WHY_HL_COM_df\"].isin([pd.NaT]), \"RET_WHY_HL_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"RET_WHY_HL_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"RET_WHY_HL_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10346\n",
      "Number of missing values after re-encoding: 91\n"
     ]
    }
   ],
   "source": [
    "# RET_WHY_IN_COM\n",
    "merged.loc[merged['RET_WHY_IN_COM_df'] == 1, \"RET_WHY_IN_COM\"] = 1\n",
    "merged.loc[(merged[\"RET_WHY_IN_COM_df\"] == 0) | (merged[\"RET_RTRD_COM_df\"] == 3), \"RET_WHY_IN_COM\"] = 0\n",
    "merged.loc[merged[\"RET_WHY_IN_COM_df\"].isin([pd.NaT]), \"RET_WHY_IN_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"RET_WHY_IN_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"RET_WHY_IN_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10346\n",
      "Number of missing values after re-encoding: 91\n"
     ]
    }
   ],
   "source": [
    "# RET_WHY_OR_COM\n",
    "merged.loc[merged['RET_WHY_OR_COM_df'] == 1, \"RET_WHY_OR_COM\"] = 1\n",
    "merged.loc[(merged[\"RET_WHY_OR_COM_df\"] == 0) | (merged[\"RET_RTRD_COM_df\"] == 3), \"RET_WHY_OR_COM\"] = 0\n",
    "merged.loc[merged[\"RET_WHY_OR_COM_df\"].isin([pd.NaT]), \"RET_WHY_OR_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"RET_WHY_OR_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"RET_WHY_OR_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10346\n",
      "Number of missing values after re-encoding: 91\n"
     ]
    }
   ],
   "source": [
    "# RET_WHY_PR_COM\n",
    "merged.loc[merged['RET_WHY_PR_COM_df'] == 1, \"RET_WHY_PR_COM\"] = 1\n",
    "merged.loc[(merged[\"RET_WHY_PR_COM_df\"] == 0) | (merged[\"RET_RTRD_COM_df\"] == 3), \"RET_WHY_PR_COM\"] = 0\n",
    "merged.loc[merged[\"RET_WHY_PR_COM_df\"].isin([pd.NaT]), \"RET_WHY_PR_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"RET_WHY_PR_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"RET_WHY_PR_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10346\n",
      "Number of missing values after re-encoding: 91\n"
     ]
    }
   ],
   "source": [
    "# RET_WHY_HO_COM\n",
    "merged.loc[merged['RET_WHY_HO_COM_df'] == 1, \"RET_WHY_HO_COM\"] = 1\n",
    "merged.loc[(merged[\"RET_WHY_HO_COM_df\"] == 0) | (merged[\"RET_RTRD_COM_df\"] == 3), \"RET_WHY_HO_COM\"] = 0\n",
    "merged.loc[merged[\"RET_WHY_HO_COM_df\"].isin([pd.NaT]), \"RET_WHY_HO_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"RET_WHY_HO_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"RET_WHY_HO_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10346\n",
      "Number of missing values after re-encoding: 91\n"
     ]
    }
   ],
   "source": [
    "# RET_WHY_ST_COM\n",
    "merged.loc[merged['RET_WHY_ST_COM_df'] == 1, \"RET_WHY_ST_COM\"] = 1\n",
    "merged.loc[(merged[\"RET_WHY_ST_COM_df\"] == 0) | (merged[\"RET_RTRD_COM_df\"] == 3), \"RET_WHY_ST_COM\"] = 0\n",
    "merged.loc[merged[\"RET_WHY_ST_COM_df\"].isin([pd.NaT]), \"RET_WHY_ST_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"RET_WHY_ST_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"RET_WHY_ST_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10346\n",
      "Number of missing values after re-encoding: 91\n"
     ]
    }
   ],
   "source": [
    "# RET_WHY_AG_COM\n",
    "merged.loc[merged['RET_WHY_AG_COM_df'] == 1, \"RET_WHY_AG_COM\"] = 1\n",
    "merged.loc[(merged[\"RET_WHY_AG_COM_df\"] == 0) | (merged[\"RET_RTRD_COM_df\"] == 3), \"RET_WHY_AG_COM\"] = 0\n",
    "merged.loc[merged[\"RET_WHY_AG_COM_df\"].isin([pd.NaT]), \"RET_WHY_AG_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"RET_WHY_AG_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"RET_WHY_AG_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 7951\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_PUBTR_NN_MCQ\n",
    "merged.loc[merged['TRA_PUBTR_NN_MCQ_df'] == 1, \"TRA_PUBTR_NN_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_PUBTR_NN_MCQ_df\"] == 0) | (merged[\"TRA_TYPTR_PUB_MCQ_df\"] == 1), \"TRA_PUBTR_NN_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_PUBTR_NN_MCQ_df\"].isin([pd.NaT]), \"TRA_PUBTR_NN_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_PUBTR_NN_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_PUBTR_NN_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 7951\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_PUBTR_PNU_MCQ\n",
    "merged.loc[merged['TRA_PUBTR_PNU_MCQ_df'] == 1, \"TRA_PUBTR_PNU_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_PUBTR_PNU_MCQ_df\"] == 0) | (merged[\"TRA_TYPTR_PUB_MCQ_df\"] == 1), \"TRA_PUBTR_PNU_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_PUBTR_PNU_MCQ_df\"].isin([pd.NaT]), \"TRA_PUBTR_PNU_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_PUBTR_PNU_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_PUBTR_PNU_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 7951\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_PUBTR_UNA_MCQ\n",
    "merged.loc[merged['TRA_PUBTR_UNA_MCQ_df'] == 1, \"TRA_PUBTR_UNA_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_PUBTR_UNA_MCQ_df\"] == 0) | (merged[\"TRA_TYPTR_PUB_MCQ_df\"] == 1), \"TRA_PUBTR_UNA_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_PUBTR_UNA_MCQ_df\"].isin([pd.NaT]), \"TRA_PUBTR_UNA_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_PUBTR_UNA_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_PUBTR_UNA_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 7951\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_PUBTR_INC_MCQ\n",
    "merged.loc[merged['TRA_PUBTR_INC_MCQ_df'] == 1, \"TRA_PUBTR_INC_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_PUBTR_INC_MCQ_df\"] == 0) | (merged[\"TRA_TYPTR_PUB_MCQ_df\"] == 1), \"TRA_PUBTR_INC_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_PUBTR_INC_MCQ_df\"].isin([pd.NaT]), \"TRA_PUBTR_INC_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_PUBTR_INC_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_PUBTR_INC_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_RA_MCQ\n",
    "merged.loc[merged['TRA_AVOID_RA_MCQ_df'] == 1, \"TRA_AVOID_RA_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_RA_MCQ_df\"] == 0), \"TRA_AVOID_RA_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_RA_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_RA_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_RA_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_RA_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_CR_MCQ\n",
    "merged.loc[merged['TRA_AVOID_CR_MCQ_df'] == 1, \"TRA_AVOID_CR_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_CR_MCQ_df\"] == 0), \"TRA_AVOID_CR_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_CR_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_CR_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_CR_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_CR_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_UN_MCQ\n",
    "merged.loc[merged['TRA_AVOID_UN_MCQ_df'] == 1, \"TRA_AVOID_UN_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_UN_MCQ_df\"] == 0), \"TRA_AVOID_UN_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_UN_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_UN_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_UN_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_UN_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_HV_MCQ\n",
    "merged.loc[merged['TRA_AVOID_HV_MCQ_df'] == 1, \"TRA_AVOID_HV_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_HV_MCQ_df\"] == 0), \"TRA_AVOID_HV_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_HV_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_HV_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_HV_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_HV_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_ML_MCQ\n",
    "merged.loc[merged['TRA_AVOID_ML_MCQ_df'] == 1, \"TRA_AVOID_ML_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_ML_MCQ_df\"] == 0), \"TRA_AVOID_ML_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_ML_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_ML_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_ML_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_ML_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_SL_MCQ\n",
    "merged.loc[merged['TRA_AVOID_SL_MCQ_df'] == 1, \"TRA_AVOID_SL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_SL_MCQ_df\"] == 0), \"TRA_AVOID_SL_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_SL_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_SL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_SL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_SL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_NL_MCQ\n",
    "merged.loc[merged['TRA_AVOID_NL_MCQ_df'] == 1, \"TRA_AVOID_NL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_NL_MCQ_df\"] == 0), \"TRA_AVOID_NL_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_NL_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_NL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_NL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_NL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_LG_MCQ\n",
    "merged.loc[merged['TRA_AVOID_LG_MCQ_df'] == 1, \"TRA_AVOID_LG_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_LG_MCQ_df\"] == 0), \"TRA_AVOID_LG_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_LG_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_LG_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_LG_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_LG_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_BS_MCQ\n",
    "merged.loc[merged['TRA_AVOID_BS_MCQ_df'] == 1, \"TRA_AVOID_BS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_BS_MCQ_df\"] == 0), \"TRA_AVOID_BS_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_BS_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_BS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_BS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_BS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_SN_MCQ\n",
    "merged.loc[merged['TRA_AVOID_SN_MCQ_df'] == 1, \"TRA_AVOID_SN_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_SN_MCQ_df\"] == 0), \"TRA_AVOID_SN_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_SN_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_SN_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_SN_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_SN_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_DW_MCQ\n",
    "merged.loc[merged['TRA_AVOID_DW_MCQ_df'] == 1, \"TRA_AVOID_DW_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_DW_MCQ_df\"] == 0), \"TRA_AVOID_DW_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_DW_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_DW_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_DW_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_DW_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_NT_MCQ\n",
    "merged.loc[merged['TRA_AVOID_NT_MCQ_df'] == 1, \"TRA_AVOID_NT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_NT_MCQ_df\"] == 0), \"TRA_AVOID_NT_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_NT_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_NT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_NT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_NT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2172\n",
      "Number of missing values after re-encoding: 2172\n"
     ]
    }
   ],
   "source": [
    "# TRA_AVOID_NONE_MCQ\n",
    "merged.loc[merged['TRA_AVOID_NONE_MCQ_df'] == 1, \"TRA_AVOID_NONE_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_AVOID_NONE_MCQ_df\"] == 0), \"TRA_AVOID_NONE_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_AVOID_NONE_MCQ_df\"].isin([pd.NaT]), \"TRA_AVOID_NONE_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_AVOID_NONE_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_AVOID_NONE_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1306\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_ACCTR_NN_MCQ\n",
    "merged.loc[merged['TRA_ACCTR_NN_MCQ_df'] == 1, \"TRA_ACCTR_NN_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_ACCTR_NN_MCQ_df\"] == 0) | (merged[\"TRA_TYPTR_ACC_MCQ_df\"] == 1), \"TRA_ACCTR_NN_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_ACCTR_NN_MCQ_df\"].isin([pd.NaT]), \"TRA_ACCTR_NN_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_ACCTR_NN_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_ACCTR_NN_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1045\n",
      "Number of missing values after re-encoding: 965\n"
     ]
    }
   ],
   "source": [
    "# PA2_SIT_BIN_MCQ\n",
    "merged.loc[merged['PA2_SIT_BIN_MCQ_df'] == 1, \"PA2_SIT_BIN_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SIT_BIN_MCQ_df\"] == 0) | (merged[\"PA2_SIT_MCQ_df\"] == 1), \"PA2_SIT_BIN_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SIT_BIN_MCQ_df\"].isin([pd.NaT]), \"PA2_SIT_BIN_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SIT_BIN_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SIT_BIN_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1045\n",
      "Number of missing values after re-encoding: 965\n"
     ]
    }
   ],
   "source": [
    "# PA2_SIT_COM_MCQ\n",
    "merged.loc[merged['PA2_SIT_COM_MCQ_df'] == 1, \"PA2_SIT_COM_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SIT_COM_MCQ_df\"] == 0) | (merged[\"PA2_SIT_MCQ_df\"] == 1), \"PA2_SIT_COM_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SIT_COM_MCQ_df\"].isin([pd.NaT]), \"PA2_SIT_COM_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SIT_COM_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SIT_COM_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1045\n",
      "Number of missing values after re-encoding: 965\n"
     ]
    }
   ],
   "source": [
    "# PA2_SIT_CRO_MCQ\n",
    "merged.loc[merged['PA2_SIT_CRO_MCQ_df'] == 1, \"PA2_SIT_CRO_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SIT_CRO_MCQ_df\"] == 0) | (merged[\"PA2_SIT_MCQ_df\"] == 1), \"PA2_SIT_CRO_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SIT_CRO_MCQ_df\"].isin([pd.NaT]), \"PA2_SIT_CRO_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SIT_CRO_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SIT_CRO_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1045\n",
      "Number of missing values after re-encoding: 965\n"
     ]
    }
   ],
   "source": [
    "# PA2_SIT_HAN_MCQ\n",
    "merged.loc[merged['PA2_SIT_HAN_MCQ_df'] == 1, \"PA2_SIT_HAN_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SIT_HAN_MCQ_df\"] == 0) | (merged[\"PA2_SIT_MCQ_df\"] == 1), \"PA2_SIT_HAN_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SIT_HAN_MCQ_df\"].isin([pd.NaT]), \"PA2_SIT_HAN_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SIT_HAN_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SIT_HAN_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1045\n",
      "Number of missing values after re-encoding: 965\n"
     ]
    }
   ],
   "source": [
    "# PA2_SIT_LIS_MCQ\n",
    "merged.loc[merged['PA2_SIT_LIS_MCQ_df'] == 1, \"PA2_SIT_LIS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SIT_LIS_MCQ_df\"] == 0) | (merged[\"PA2_SIT_MCQ_df\"] == 1), \"PA2_SIT_LIS_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SIT_LIS_MCQ_df\"].isin([pd.NaT]), \"PA2_SIT_LIS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SIT_LIS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SIT_LIS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1045\n",
      "Number of missing values after re-encoding: 965\n"
     ]
    }
   ],
   "source": [
    "# PA2_SIT_REA_MCQ\n",
    "merged.loc[merged['PA2_SIT_REA_MCQ_df'] == 1, \"PA2_SIT_REA_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SIT_REA_MCQ_df\"] == 0) | (merged[\"PA2_SIT_MCQ_df\"] == 1), \"PA2_SIT_REA_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SIT_REA_MCQ_df\"].isin([pd.NaT]), \"PA2_SIT_REA_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SIT_REA_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SIT_REA_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1045\n",
      "Number of missing values after re-encoding: 965\n"
     ]
    }
   ],
   "source": [
    "# PA2_SIT_VIS_MCQ\n",
    "merged.loc[merged['PA2_SIT_VIS_MCQ_df'] == 1, \"PA2_SIT_VIS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SIT_VIS_MCQ_df\"] == 0) | (merged[\"PA2_SIT_MCQ_df\"] == 1), \"PA2_SIT_VIS_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SIT_VIS_MCQ_df\"].isin([pd.NaT]), \"PA2_SIT_VIS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SIT_VIS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SIT_VIS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1045\n",
      "Number of missing values after re-encoding: 965\n"
     ]
    }
   ],
   "source": [
    "# PA2_SIT_TV_MCQ\n",
    "merged.loc[merged['PA2_SIT_TV_MCQ_df'] == 1, \"PA2_SIT_TV_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SIT_TV_MCQ_df\"] == 0) | (merged[\"PA2_SIT_MCQ_df\"] == 1), \"PA2_SIT_TV_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SIT_TV_MCQ_df\"].isin([pd.NaT]), \"PA2_SIT_TV_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SIT_TV_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SIT_TV_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1045\n",
      "Number of missing values after re-encoding: 965\n"
     ]
    }
   ],
   "source": [
    "# PA2_SIT_OT_MCQ\n",
    "merged.loc[merged['PA2_SIT_OT_MCQ_df'] == 1, \"PA2_SIT_OT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_SIT_OT_MCQ_df\"] == 0) | (merged[\"PA2_SIT_MCQ_df\"] == 1), \"PA2_SIT_OT_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_SIT_OT_MCQ_df\"].isin([pd.NaT]), \"PA2_SIT_OT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_SIT_OT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_SIT_OT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_TTH_MCQ\n",
    "merged.loc[merged['ORH_EXP_TTH_MCQ_df'] == 1, \"ORH_EXP_TTH_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_TTH_MCQ_df\"] == 0), \"ORH_EXP_TTH_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_TTH_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_TTH_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_TTH_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_TTH_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_CHW_MCQ\n",
    "merged.loc[merged['ORH_EXP_CHW_MCQ_df'] == 1, \"ORH_EXP_CHW_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_CHW_MCQ_df\"] == 0), \"ORH_EXP_CHW_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_CHW_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_CHW_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_CHW_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_CHW_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_DNU_MCQ\n",
    "merged.loc[merged['ORH_EXP_DNU_MCQ_df'] == 1, \"ORH_EXP_DNU_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_DNU_MCQ_df\"] == 0), \"ORH_EXP_DNU_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_DNU_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_DNU_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_DNU_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_DNU_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_DNL_MCQ\n",
    "merged.loc[merged['ORH_EXP_DNL_MCQ_df'] == 1, \"ORH_EXP_DNL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_DNL_MCQ_df\"] == 0), \"ORH_EXP_DNL_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_DNL_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_DNL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_DNL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_DNL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_SWL_MCQ\n",
    "merged.loc[merged['ORH_EXP_SWL_MCQ_df'] == 1, \"ORH_EXP_SWL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_SWL_MCQ_df\"] == 0), \"ORH_EXP_SWL_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_SWL_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_SWL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_SWL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_SWL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_DRM_MCQ\n",
    "merged.loc[merged['ORH_EXP_DRM_MCQ_df'] == 1, \"ORH_EXP_DRM_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_DRM_MCQ_df\"] == 0), \"ORH_EXP_DRM_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_DRM_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_DRM_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_DRM_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_DRM_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_JWS_MCQ\n",
    "merged.loc[merged['ORH_EXP_JWS_MCQ_df'] == 1, \"ORH_EXP_JWS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_JWS_MCQ_df\"] == 0), \"ORH_EXP_JWS_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_JWS_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_JWS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_JWS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_JWS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_JJP_MCQ\n",
    "merged.loc[merged['ORH_EXP_JJP_MCQ_df'] == 1, \"ORH_EXP_JJP_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_JJP_MCQ_df\"] == 0), \"ORH_EXP_JJP_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_JJP_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_JJP_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_JJP_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_JJP_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_NTD_MCQ\n",
    "merged.loc[merged['ORH_EXP_NTD_MCQ_df'] == 1, \"ORH_EXP_NTD_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_NTD_MCQ_df\"] == 0), \"ORH_EXP_NTD_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_NTD_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_NTD_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_NTD_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_NTD_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_NTL_MCQ\n",
    "merged.loc[merged['ORH_EXP_NTL_MCQ_df'] == 1, \"ORH_EXP_NTL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_NTL_MCQ_df\"] == 0), \"ORH_EXP_NTL_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_NTL_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_NTL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_NTL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_NTL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_NTB_MCQ\n",
    "merged.loc[merged['ORH_EXP_NTB_MCQ_df'] == 1, \"ORH_EXP_NTB_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_NTB_MCQ_df\"] == 0), \"ORH_EXP_NTB_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_NTB_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_NTB_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_NTB_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_NTB_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_GUMS_MCQ\n",
    "merged.loc[merged['ORH_EXP_GUMS_MCQ_df'] == 1, \"ORH_EXP_GUMS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_GUMS_MCQ_df\"] == 0), \"ORH_EXP_GUMS_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_GUMS_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_GUMS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_GUMS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_GUMS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_GUMB_MCQ\n",
    "merged.loc[merged['ORH_EXP_GUMB_MCQ_df'] == 1, \"ORH_EXP_GUMB_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_GUMB_MCQ_df\"] == 0), \"ORH_EXP_GUMB_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_GUMB_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_GUMB_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_GUMB_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_GUMB_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_BB_MCQ\n",
    "merged.loc[merged['ORH_EXP_BB_MCQ_df'] == 1, \"ORH_EXP_BB_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_BB_MCQ_df\"] == 0), \"ORH_EXP_BB_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_BB_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_BB_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_BB_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_BB_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_NONE_MCQ\n",
    "merged.loc[merged['ORH_EXP_NONE_MCQ_df'] == 1, \"ORH_EXP_NONE_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_NONE_MCQ_df\"] == 0), \"ORH_EXP_NONE_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_NONE_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_NONE_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_NONE_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_NONE_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ORH_EXP_OT_MCQ\n",
    "merged.loc[merged['ORH_EXP_OT_MCQ_df'] == 1, \"ORH_EXP_OT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_EXP_OT_MCQ_df\"] == 0), \"ORH_EXP_OT_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_EXP_OT_MCQ_df\"].isin([pd.NaT]), \"ORH_EXP_OT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_EXP_OT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_EXP_OT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ENV_HMPRB_NOI_MCQ\n",
    "merged.loc[merged['ENV_HMPRB_NOI_MCQ_df'] == 1, \"ENV_HMPRB_NOI_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ENV_HMPRB_NOI_MCQ_df\"] == 0), \"ENV_HMPRB_NOI_MCQ\"] = 0\n",
    "merged.loc[merged[\"ENV_HMPRB_NOI_MCQ_df\"].isin([pd.NaT]), \"ENV_HMPRB_NOI_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ENV_HMPRB_NOI_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ENV_HMPRB_NOI_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ENV_HMPRB_LEA_MCQ\n",
    "merged.loc[merged['ENV_HMPRB_LEA_MCQ_df'] == 1, \"ENV_HMPRB_LEA_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ENV_HMPRB_LEA_MCQ_df\"] == 0), \"ENV_HMPRB_LEA_MCQ\"] = 0\n",
    "merged.loc[merged[\"ENV_HMPRB_LEA_MCQ_df\"].isin([pd.NaT]), \"ENV_HMPRB_LEA_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ENV_HMPRB_LEA_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ENV_HMPRB_LEA_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ENV_HMPRB_EP_MCQ\n",
    "merged.loc[merged['ENV_HMPRB_EP_MCQ_df'] == 1, \"ENV_HMPRB_EP_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ENV_HMPRB_EP_MCQ_df\"] == 0), \"ENV_HMPRB_EP_MCQ\"] = 0\n",
    "merged.loc[merged[\"ENV_HMPRB_EP_MCQ_df\"].isin([pd.NaT]), \"ENV_HMPRB_EP_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ENV_HMPRB_EP_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ENV_HMPRB_EP_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ENV_HMPRB_MAI_MCQ\n",
    "merged.loc[merged['ENV_HMPRB_MAI_MCQ_df'] == 1, \"ENV_HMPRB_MAI_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ENV_HMPRB_MAI_MCQ_df\"] == 0), \"ENV_HMPRB_MAI_MCQ\"] = 0\n",
    "merged.loc[merged[\"ENV_HMPRB_MAI_MCQ_df\"].isin([pd.NaT]), \"ENV_HMPRB_MAI_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ENV_HMPRB_MAI_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ENV_HMPRB_MAI_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ENV_HMPRB_INF_MCQ\n",
    "merged.loc[merged['ENV_HMPRB_INF_MCQ_df'] == 1, \"ENV_HMPRB_INF_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ENV_HMPRB_INF_MCQ_df\"] == 0), \"ENV_HMPRB_INF_MCQ\"] = 0\n",
    "merged.loc[merged[\"ENV_HMPRB_INF_MCQ_df\"].isin([pd.NaT]), \"ENV_HMPRB_INF_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ENV_HMPRB_INF_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ENV_HMPRB_INF_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# ENV_HMPRB_NONE_MCQ\n",
    "merged.loc[merged['ENV_HMPRB_NONE_MCQ_df'] == 1, \"ENV_HMPRB_NONE_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ENV_HMPRB_NONE_MCQ_df\"] == 0), \"ENV_HMPRB_NONE_MCQ\"] = 0\n",
    "merged.loc[merged[\"ENV_HMPRB_NONE_MCQ_df\"].isin([pd.NaT]), \"ENV_HMPRB_NONE_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ENV_HMPRB_NONE_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ENV_HMPRB_NONE_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TYPTR_PAS_MCQ\n",
    "merged.loc[merged['TRA_TYPTR_PAS_MCQ_df'] == 1, \"TRA_TYPTR_PAS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TYPTR_PAS_MCQ_df\"] == 0), \"TRA_TYPTR_PAS_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TYPTR_PAS_MCQ_df\"].isin([pd.NaT]), \"TRA_TYPTR_PAS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TYPTR_PAS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TYPTR_PAS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TYPTR_TAX_MCQ\n",
    "merged.loc[merged['TRA_TYPTR_TAX_MCQ_df'] == 1, \"TRA_TYPTR_TAX_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TYPTR_TAX_MCQ_df\"] == 0), \"TRA_TYPTR_TAX_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TYPTR_TAX_MCQ_df\"].isin([pd.NaT]), \"TRA_TYPTR_TAX_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TYPTR_TAX_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TYPTR_TAX_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TYPTR_PUB_MCQ\n",
    "merged.loc[merged['TRA_TYPTR_PUB_MCQ_df'] == 1, \"TRA_TYPTR_PUB_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TYPTR_PUB_MCQ_df\"] == 0), \"TRA_TYPTR_PUB_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TYPTR_PUB_MCQ_df\"].isin([pd.NaT]), \"TRA_TYPTR_PUB_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TYPTR_PUB_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TYPTR_PUB_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TYPTR_CYC_MCQ\n",
    "merged.loc[merged['TRA_TYPTR_CYC_MCQ_df'] == 1, \"TRA_TYPTR_CYC_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TYPTR_CYC_MCQ_df\"] == 0), \"TRA_TYPTR_CYC_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TYPTR_CYC_MCQ_df\"].isin([pd.NaT]), \"TRA_TYPTR_CYC_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TYPTR_CYC_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TYPTR_CYC_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TYPTR_WAL_MCQ\n",
    "merged.loc[merged['TRA_TYPTR_WAL_MCQ_df'] == 1, \"TRA_TYPTR_WAL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TYPTR_WAL_MCQ_df\"] == 0), \"TRA_TYPTR_WAL_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TYPTR_WAL_MCQ_df\"].isin([pd.NaT]), \"TRA_TYPTR_WAL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TYPTR_WAL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TYPTR_WAL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TYPTR_NONE_MCQ\n",
    "merged.loc[merged['TRA_TYPTR_NONE_MCQ_df'] == 1, \"TRA_TYPTR_NONE_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TYPTR_NONE_MCQ_df\"] == 0), \"TRA_TYPTR_NONE_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TYPTR_NONE_MCQ_df\"].isin([pd.NaT]), \"TRA_TYPTR_NONE_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TYPTR_NONE_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TYPTR_NONE_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TRIP_WK_MCQ\n",
    "merged.loc[merged['TRA_TRIP_WK_MCQ_df'] == 1, \"TRA_TRIP_WK_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TRIP_WK_MCQ_df\"] == 0), \"TRA_TRIP_WK_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TRIP_WK_MCQ_df\"].isin([pd.NaT]), \"TRA_TRIP_WK_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TRIP_WK_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TRIP_WK_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TRIP_BK_MCQ\n",
    "merged.loc[merged['TRA_TRIP_BK_MCQ_df'] == 1, \"TRA_TRIP_BK_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TRIP_BK_MCQ_df\"] == 0), \"TRA_TRIP_BK_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TRIP_BK_MCQ_df\"].isin([pd.NaT]), \"TRA_TRIP_BK_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TRIP_BK_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TRIP_BK_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TRIP_MD_MCQ\n",
    "merged.loc[merged['TRA_TRIP_MD_MCQ_df'] == 1, \"TRA_TRIP_MD_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TRIP_MD_MCQ_df\"] == 0), \"TRA_TRIP_MD_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TRIP_MD_MCQ_df\"].isin([pd.NaT]), \"TRA_TRIP_MD_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TRIP_MD_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TRIP_MD_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TRIP_GR_MCQ\n",
    "merged.loc[merged['TRA_TRIP_GR_MCQ_df'] == 1, \"TRA_TRIP_GR_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TRIP_GR_MCQ_df\"] == 0), \"TRA_TRIP_GR_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TRIP_GR_MCQ_df\"].isin([pd.NaT]), \"TRA_TRIP_GR_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TRIP_GR_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TRIP_GR_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TRIP_RI_MCQ\n",
    "merged.loc[merged['TRA_TRIP_RI_MCQ_df'] == 1, \"TRA_TRIP_RI_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TRIP_RI_MCQ_df\"] == 0), \"TRA_TRIP_RI_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TRIP_RI_MCQ_df\"].isin([pd.NaT]), \"TRA_TRIP_RI_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TRIP_RI_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TRIP_RI_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TRIP_RO_MCQ\n",
    "merged.loc[merged['TRA_TRIP_RO_MCQ_df'] == 1, \"TRA_TRIP_RO_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TRIP_RO_MCQ_df\"] == 0), \"TRA_TRIP_RO_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TRIP_RO_MCQ_df\"].isin([pd.NaT]), \"TRA_TRIP_RO_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TRIP_RO_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TRIP_RO_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TRIP_CH_MCQ\n",
    "merged.loc[merged['TRA_TRIP_CH_MCQ_df'] == 1, \"TRA_TRIP_CH_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TRIP_CH_MCQ_df\"] == 0), \"TRA_TRIP_CH_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TRIP_CH_MCQ_df\"].isin([pd.NaT]), \"TRA_TRIP_CH_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TRIP_CH_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TRIP_CH_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TRIP_FM_MCQ\n",
    "merged.loc[merged['TRA_TRIP_FM_MCQ_df'] == 1, \"TRA_TRIP_FM_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TRIP_FM_MCQ_df\"] == 0), \"TRA_TRIP_FM_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TRIP_FM_MCQ_df\"].isin([pd.NaT]), \"TRA_TRIP_FM_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TRIP_FM_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TRIP_FM_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TRIP_SO_MCQ\n",
    "merged.loc[merged['TRA_TRIP_SO_MCQ_df'] == 1, \"TRA_TRIP_SO_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TRIP_SO_MCQ_df\"] == 0), \"TRA_TRIP_SO_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TRIP_SO_MCQ_df\"].isin([pd.NaT]), \"TRA_TRIP_SO_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TRIP_SO_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TRIP_SO_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_TRIP_OT_MCQ\n",
    "merged.loc[merged['TRA_TRIP_OT_MCQ_df'] == 1, \"TRA_TRIP_OT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_TRIP_OT_MCQ_df\"] == 0), \"TRA_TRIP_OT_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_TRIP_OT_MCQ_df\"].isin([pd.NaT]), \"TRA_TRIP_OT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_TRIP_OT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_TRIP_OT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_CMNTY_CLI_MCQ\n",
    "merged.loc[merged['TRA_CMNTY_CLI_MCQ_df'] == 1, \"TRA_CMNTY_CLI_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_CMNTY_CLI_MCQ_df\"] == 0), \"TRA_CMNTY_CLI_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_CMNTY_CLI_MCQ_df\"].isin([pd.NaT]), \"TRA_CMNTY_CLI_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_CMNTY_CLI_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_CMNTY_CLI_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_CMNTY_RET_MCQ\n",
    "merged.loc[merged['TRA_CMNTY_RET_MCQ_df'] == 1, \"TRA_CMNTY_RET_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_CMNTY_RET_MCQ_df\"] == 0), \"TRA_CMNTY_RET_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_CMNTY_RET_MCQ_df\"].isin([pd.NaT]), \"TRA_CMNTY_RET_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_CMNTY_RET_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_CMNTY_RET_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_CMNTY_FAM_MCQ\n",
    "merged.loc[merged['TRA_CMNTY_FAM_MCQ_df'] == 1, \"TRA_CMNTY_FAM_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_CMNTY_FAM_MCQ_df\"] == 0), \"TRA_CMNTY_FAM_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_CMNTY_FAM_MCQ_df\"].isin([pd.NaT]), \"TRA_CMNTY_FAM_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_CMNTY_FAM_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_CMNTY_FAM_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_CMNTY_HOU_MCQ\n",
    "merged.loc[merged['TRA_CMNTY_HOU_MCQ_df'] == 1, \"TRA_CMNTY_HOU_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_CMNTY_HOU_MCQ_df\"] == 0), \"TRA_CMNTY_HOU_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_CMNTY_HOU_MCQ_df\"].isin([pd.NaT]), \"TRA_CMNTY_HOU_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_CMNTY_HOU_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_CMNTY_HOU_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_CMNTY_REC_MCQ\n",
    "merged.loc[merged['TRA_CMNTY_REC_MCQ_df'] == 1, \"TRA_CMNTY_REC_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_CMNTY_REC_MCQ_df\"] == 0), \"TRA_CMNTY_REC_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_CMNTY_REC_MCQ_df\"].isin([pd.NaT]), \"TRA_CMNTY_REC_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_CMNTY_REC_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_CMNTY_REC_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_CMNTY_COS_MCQ\n",
    "merged.loc[merged['TRA_CMNTY_COS_MCQ_df'] == 1, \"TRA_CMNTY_COS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_CMNTY_COS_MCQ_df\"] == 0), \"TRA_CMNTY_COS_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_CMNTY_COS_MCQ_df\"].isin([pd.NaT]), \"TRA_CMNTY_COS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_CMNTY_COS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_CMNTY_COS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# TRA_CMNTY_EMP_MCQ\n",
    "merged.loc[merged['TRA_CMNTY_EMP_MCQ_df'] == 1, \"TRA_CMNTY_EMP_MCQ\"] = 1\n",
    "merged.loc[(merged[\"TRA_CMNTY_EMP_MCQ_df\"] == 0), \"TRA_CMNTY_EMP_MCQ\"] = 0\n",
    "merged.loc[merged[\"TRA_CMNTY_EMP_MCQ_df\"].isin([pd.NaT]), \"TRA_CMNTY_EMP_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"TRA_CMNTY_EMP_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"TRA_CMNTY_EMP_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_SVNGS_ACC_MCQ\n",
    "merged.loc[merged['WEA_SVNGS_ACC_MCQ_df'] == 1, \"WEA_SVNGS_ACC_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_SVNGS_ACC_MCQ_df\"] == 0), \"WEA_SVNGS_ACC_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_SVNGS_ACC_MCQ_df\"].isin([pd.NaT]), \"WEA_SVNGS_ACC_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_SVNGS_ACC_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_SVNGS_ACC_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_SVNGS_RRSP_MCQ\n",
    "merged.loc[merged['WEA_SVNGS_RRSP_MCQ_df'] == 1, \"WEA_SVNGS_RRSP_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_SVNGS_RRSP_MCQ_df\"] == 0), \"WEA_SVNGS_RRSP_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_SVNGS_RRSP_MCQ_df\"].isin([pd.NaT]), \"WEA_SVNGS_RRSP_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_SVNGS_RRSP_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_SVNGS_RRSP_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_SVNGS_INV_MCQ\n",
    "merged.loc[merged['WEA_SVNGS_INV_MCQ_df'] == 1, \"WEA_SVNGS_INV_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_SVNGS_INV_MCQ_df\"] == 0), \"WEA_SVNGS_INV_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_SVNGS_INV_MCQ_df\"].isin([pd.NaT]), \"WEA_SVNGS_INV_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_SVNGS_INV_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_SVNGS_INV_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_ASSETS_HSE_MCQ\n",
    "merged.loc[merged['WEA_ASSETS_HSE_MCQ_df'] == 1, \"WEA_ASSETS_HSE_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_ASSETS_HSE_MCQ_df\"] == 0), \"WEA_ASSETS_HSE_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_ASSETS_HSE_MCQ_df\"].isin([pd.NaT]), \"WEA_ASSETS_HSE_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_ASSETS_HSE_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_ASSETS_HSE_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_ASSETS_PRES_MCQ\n",
    "merged.loc[merged['WEA_ASSETS_PRES_MCQ_df'] == 1, \"WEA_ASSETS_PRES_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_ASSETS_PRES_MCQ_df\"] == 0), \"WEA_ASSETS_PRES_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_ASSETS_PRES_MCQ_df\"].isin([pd.NaT]), \"WEA_ASSETS_PRES_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_ASSETS_PRES_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_ASSETS_PRES_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_ASSETS_FBS_MCQ\n",
    "merged.loc[merged['WEA_ASSETS_FBS_MCQ_df'] == 1, \"WEA_ASSETS_FBS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_ASSETS_FBS_MCQ_df\"] == 0), \"WEA_ASSETS_FBS_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_ASSETS_FBS_MCQ_df\"].isin([pd.NaT]), \"WEA_ASSETS_FBS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_ASSETS_FBS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_ASSETS_FBS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_ASSETS_OTL_MCQ\n",
    "merged.loc[merged['WEA_ASSETS_OTL_MCQ_df'] == 1, \"WEA_ASSETS_OTL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_ASSETS_OTL_MCQ_df\"] == 0), \"WEA_ASSETS_OTL_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_ASSETS_OTL_MCQ_df\"].isin([pd.NaT]), \"WEA_ASSETS_OTL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_ASSETS_OTL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_ASSETS_OTL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_ASSETS_MOWD_MCQ\n",
    "merged.loc[merged['WEA_ASSETS_MOWD_MCQ_df'] == 1, \"WEA_ASSETS_MOWD_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_ASSETS_MOWD_MCQ_df\"] == 0), \"WEA_ASSETS_MOWD_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_ASSETS_MOWD_MCQ_df\"].isin([pd.NaT]), \"WEA_ASSETS_MOWD_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_ASSETS_MOWD_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_ASSETS_MOWD_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_ASSETS_CINH_MCQ\n",
    "merged.loc[merged['WEA_ASSETS_CINH_MCQ_df'] == 1, \"WEA_ASSETS_CINH_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_ASSETS_CINH_MCQ_df\"] == 0), \"WEA_ASSETS_CINH_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_ASSETS_CINH_MCQ_df\"].isin([pd.NaT]), \"WEA_ASSETS_CINH_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_ASSETS_CINH_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_ASSETS_CINH_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_ASSETS_NONE_MCQ\n",
    "merged.loc[merged['WEA_ASSETS_NONE_MCQ_df'] == 1, \"WEA_ASSETS_NONE_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_ASSETS_NONE_MCQ_df\"] == 0), \"WEA_ASSETS_NONE_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_ASSETS_NONE_MCQ_df\"].isin([pd.NaT]), \"WEA_ASSETS_NONE_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_ASSETS_NONE_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_ASSETS_NONE_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_ASSETS_OT_MCQ\n",
    "merged.loc[merged['WEA_ASSETS_OT_MCQ_df'] == 1, \"WEA_ASSETS_OT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_ASSETS_OT_MCQ_df\"] == 0), \"WEA_ASSETS_OT_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_ASSETS_OT_MCQ_df\"].isin([pd.NaT]), \"WEA_ASSETS_OT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_ASSETS_OT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_ASSETS_OT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_DEBT_CCRD_MCQ\n",
    "merged.loc[merged['WEA_DEBT_CCRD_MCQ_df'] == 1, \"WEA_DEBT_CCRD_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_DEBT_CCRD_MCQ_df\"] == 0), \"WEA_DEBT_CCRD_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_DEBT_CCRD_MCQ_df\"].isin([pd.NaT]), \"WEA_DEBT_CCRD_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_DEBT_CCRD_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_DEBT_CCRD_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_DEBT_LNS_MCQ\n",
    "merged.loc[merged['WEA_DEBT_LNS_MCQ_df'] == 1, \"WEA_DEBT_LNS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_DEBT_LNS_MCQ_df\"] == 0), \"WEA_DEBT_LNS_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_DEBT_LNS_MCQ_df\"].isin([pd.NaT]), \"WEA_DEBT_LNS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_DEBT_LNS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_DEBT_LNS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_DEBT_NONE_MCQ\n",
    "merged.loc[merged['WEA_DEBT_NONE_MCQ_df'] == 1, \"WEA_DEBT_NONE_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_DEBT_NONE_MCQ_df\"] == 0), \"WEA_DEBT_NONE_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_DEBT_NONE_MCQ_df\"].isin([pd.NaT]), \"WEA_DEBT_NONE_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_DEBT_NONE_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_DEBT_NONE_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_THNGS_FOD_MCQ\n",
    "merged.loc[merged['WEA_THNGS_FOD_MCQ_df'] == 1, \"WEA_THNGS_FOD_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_THNGS_FOD_MCQ_df\"] == 0), \"WEA_THNGS_FOD_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_THNGS_FOD_MCQ_df\"].isin([pd.NaT]), \"WEA_THNGS_FOD_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_THNGS_FOD_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_THNGS_FOD_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_THNGS_FFO_MCQ\n",
    "merged.loc[merged['WEA_THNGS_FFO_MCQ_df'] == 1, \"WEA_THNGS_FFO_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_THNGS_FFO_MCQ_df\"] == 0), \"WEA_THNGS_FFO_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_THNGS_FFO_MCQ_df\"].isin([pd.NaT]), \"WEA_THNGS_FFO_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_THNGS_FFO_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_THNGS_FFO_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_THNGS_POF_MCQ\n",
    "merged.loc[merged['WEA_THNGS_POF_MCQ_df'] == 1, \"WEA_THNGS_POF_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_THNGS_POF_MCQ_df\"] == 0), \"WEA_THNGS_POF_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_THNGS_POF_MCQ_df\"].isin([pd.NaT]), \"WEA_THNGS_POF_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_THNGS_POF_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_THNGS_POF_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_THNGS_HMR_MCQ\n",
    "merged.loc[merged['WEA_THNGS_HMR_MCQ_df'] == 1, \"WEA_THNGS_HMR_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_THNGS_HMR_MCQ_df\"] == 0), \"WEA_THNGS_HMR_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_THNGS_HMR_MCQ_df\"].isin([pd.NaT]), \"WEA_THNGS_HMR_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_THNGS_HMR_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_THNGS_HMR_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_THNGS_REL_MCQ\n",
    "merged.loc[merged['WEA_THNGS_REL_MCQ_df'] == 1, \"WEA_THNGS_REL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_THNGS_REL_MCQ_df\"] == 0), \"WEA_THNGS_REL_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_THNGS_REL_MCQ_df\"].isin([pd.NaT]), \"WEA_THNGS_REL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_THNGS_REL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_THNGS_REL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_THNGS_TRSP_MCQ\n",
    "merged.loc[merged['WEA_THNGS_TRSP_MCQ_df'] == 1, \"WEA_THNGS_TRSP_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_THNGS_TRSP_MCQ_df\"] == 0), \"WEA_THNGS_TRSP_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_THNGS_TRSP_MCQ_df\"].isin([pd.NaT]), \"WEA_THNGS_TRSP_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_THNGS_TRSP_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_THNGS_TRSP_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_THNGS_PRES_MCQ\n",
    "merged.loc[merged['WEA_THNGS_PRES_MCQ_df'] == 1, \"WEA_THNGS_PRES_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_THNGS_PRES_MCQ_df\"] == 0), \"WEA_THNGS_PRES_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_THNGS_PRES_MCQ_df\"].isin([pd.NaT]), \"WEA_THNGS_PRES_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_THNGS_PRES_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_THNGS_PRES_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_THNGS_HLDY_MCQ\n",
    "merged.loc[merged['WEA_THNGS_HLDY_MCQ_df'] == 1, \"WEA_THNGS_HLDY_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_THNGS_HLDY_MCQ_df\"] == 0), \"WEA_THNGS_HLDY_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_THNGS_HLDY_MCQ_df\"].isin([pd.NaT]), \"WEA_THNGS_HLDY_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_THNGS_HLDY_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_THNGS_HLDY_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_THNGS_TRSLF_MCQ\n",
    "merged.loc[merged['WEA_THNGS_TRSLF_MCQ_df'] == 1, \"WEA_THNGS_TRSLF_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_THNGS_TRSLF_MCQ_df\"] == 0), \"WEA_THNGS_TRSLF_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_THNGS_TRSLF_MCQ_df\"].isin([pd.NaT]), \"WEA_THNGS_TRSLF_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_THNGS_TRSLF_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_THNGS_TRSLF_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 947\n"
     ]
    }
   ],
   "source": [
    "# WEA_THNGS_NONE_MCQ\n",
    "merged.loc[merged['WEA_THNGS_NONE_MCQ_df'] == 1, \"WEA_THNGS_NONE_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_THNGS_NONE_MCQ_df\"] == 0), \"WEA_THNGS_NONE_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_THNGS_NONE_MCQ_df\"].isin([pd.NaT]), \"WEA_THNGS_NONE_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_THNGS_NONE_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_THNGS_NONE_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 22812\n",
      "Number of missing values after re-encoding: 52\n"
     ]
    }
   ],
   "source": [
    "# CAO_COFMAM_COM\n",
    "merged.loc[merged['CAO_COFMAM_COM_df'] == 1, \"CAO_COFMAM_COM\"] = 1\n",
    "merged.loc[(merged[\"CAO_COFMAM_COM_df\"] == 2) | (merged[\"CAO_COFAM_COM_df\"] == 2) | (merged[\"CAO_COFPY_COM_df\"] == 2), \"CAO_COFMAM_COM\"] = 0\n",
    "merged.loc[merged[\"CAO_COFMAM_COM_df\"].isin([pd.NaT,8,9]), \"CAO_COFMAM_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"CAO_COFMAM_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"CAO_COFMAM_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 22317\n",
      "Number of missing values after re-encoding: 0\n"
     ]
    }
   ],
   "source": [
    "# ICQ_SMOKE24H_COM\n",
    "merged.loc[merged['ICQ_SMOKE24H_COM_df'] == 1, \"ICQ_SMOKE24H_COM\"] = 1\n",
    "merged.loc[(merged[\"ICQ_SMOKE24H_COM_df\"] == 2) | (merged[\"ICQ_SMOKE_COM_df\"] == 2) | (merged[\"ICQ_SMOKE_COM_df\"] == 3), \"ICQ_SMOKE24H_COM\"] = 0\n",
    "merged.loc[merged[\"ICQ_SMOKE24H_COM_df\"].isin([pd.NaT]), \"ICQ_SMOKE24H_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ICQ_SMOKE24H_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ICQ_SMOKE24H_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 22048\n",
      "Number of missing values after re-encoding: 1022\n"
     ]
    }
   ],
   "source": [
    "# OAR_MED_COM\n",
    "merged.loc[merged['OAR_MED_COM_df'] == 1, \"OAR_MED_COM\"] = 1\n",
    "merged.loc[(merged[\"OAR_MED_COM_df\"] == 2) | ((merged[\"CCC_RA_COM_df\"] == 2) & (merged[\"CCC_ARTOT_COM_df\"] == 2)), \"OAR_MED_COM\"] = 0\n",
    "merged.loc[merged[\"OAR_MED_COM_df\"].isin([pd.NaT]), \"OAR_MED_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"OAR_MED_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"OAR_MED_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 20550\n",
      "Number of missing values after re-encoding: 53\n"
     ]
    }
   ],
   "source": [
    "# CAO_PHLEGMPY_COM\n",
    "merged.loc[merged['CAO_PHLEGMPY_COM_df'] == 1, \"CAO_PHLEGMPY_COM\"] = 1\n",
    "merged.loc[(merged[\"CAO_PHLEGMPY_COM_df\"] == 2) | (merged[\"CAO_COFPY_COM_df\"] == 2), \"CAO_PHLEGMPY_COM\"] = 0\n",
    "merged.loc[merged[\"CAO_PHLEGMPY_COM_df\"].isin([pd.NaT,8,9]), \"CAO_PHLEGMPY_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"CAO_PHLEGMPY_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"CAO_PHLEGMPY_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 20550\n",
      "Number of missing values after re-encoding: 47\n"
     ]
    }
   ],
   "source": [
    "# CAO_COFAM_COM\n",
    "merged.loc[merged['CAO_COFAM_COM_df'] == 1, \"CAO_COFAM_COM\"] = 1\n",
    "merged.loc[(merged[\"CAO_COFAM_COM_df\"] == 2) | (merged[\"CAO_COFPY_COM_df\"] == 2), \"CAO_COFAM_COM\"] = 0\n",
    "merged.loc[merged[\"CAO_COFAM_COM_df\"].isin([pd.NaT,8,9]), \"CAO_COFAM_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"CAO_COFAM_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"CAO_COFAM_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 19440\n",
      "Number of missing values after re-encoding: 73\n"
     ]
    }
   ],
   "source": [
    "# OSA_NBFNG_COM\n",
    "merged.loc[merged['OSA_NBFNG_COM_df'] == 1, \"OSA_NBFNG_COM\"] = 1\n",
    "merged.loc[(merged[\"OSA_NBFNG_COM_df\"] == 2) | (merged[\"OSA_LGJNT_COM_df\"] == 2), \"OSA_NBFNG_COM\"] = 0\n",
    "merged.loc[merged[\"OSA_NBFNG_COM_df\"].isin([pd.NaT,8,9]), \"OSA_NBFNG_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"OSA_NBFNG_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"OSA_NBFNG_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 16270\n",
      "Number of missing values after re-encoding: 78\n"
     ]
    }
   ],
   "source": [
    "# OST_BCKPPM_COM\n",
    "merged.loc[merged['OST_BCKPPM_COM_df'] == 1, \"OST_BCKPPM_COM\"] = 1\n",
    "merged.loc[(merged[\"OST_BCKPPM_COM_df\"] == 2) | (merged[\"OST_BP_COM_df\"] == 2), \"OST_BCKPPM_COM\"] = 0\n",
    "merged.loc[merged[\"OST_BCKPPM_COM_df\"].isin([pd.NaT,8,9]), \"OST_BCKPPM_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"OST_BCKPPM_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"OST_BCKPPM_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 14550\n",
      "Number of missing values after re-encoding: 14882\n"
     ]
    }
   ],
   "source": [
    "# SLE_LGIMPR_COM\n",
    "merged.loc[merged['SLE_LGIMPR_COM_df'] == 1, \"SLE_LGIMPR_COM\"] = 1\n",
    "merged.loc[(merged[\"SLE_LGIMPR_COM_df\"] == 2), \"SLE_LGIMPR_COM\"] = 0\n",
    "merged.loc[merged[\"SLE_LGIMPR_COM_df\"].isin([pd.NaT,8,9]), \"SLE_LGIMPR_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SLE_LGIMPR_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SLE_LGIMPR_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 14550\n",
      "Number of missing values after re-encoding: 14768\n"
     ]
    }
   ],
   "source": [
    "# SLE_LGEVE_COM\n",
    "merged.loc[merged['SLE_LGEVE_COM_df'] == 1, \"SLE_LGEVE_COM\"] = 1\n",
    "merged.loc[(merged[\"SLE_LGEVE_COM_df\"] == 2), \"SLE_LGEVE_COM\"] = 0\n",
    "merged.loc[merged[\"SLE_LGEVE_COM_df\"].isin([pd.NaT,8,9]), \"SLE_LGEVE_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SLE_LGEVE_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SLE_LGEVE_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 14959\n",
      "Number of missing values after re-encoding: 14968\n"
     ]
    }
   ],
   "source": [
    "# LBF_LGEVER_COM\n",
    "merged.loc[merged['LBF_LGEVER_COM_df'] == 1, \"LBF_LGEVER_COM\"] = 1\n",
    "merged.loc[(merged[\"LBF_LGEVER_COM_df\"] == 2), \"LBF_LGEVER_COM\"] = 0\n",
    "merged.loc[merged[\"LBF_LGEVER_COM_df\"].isin([pd.NaT,8,9]), \"LBF_LGEVER_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"LBF_LGEVER_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"LBF_LGEVER_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 14056\n",
      "Number of missing values after re-encoding: 14057\n"
     ]
    }
   ],
   "source": [
    "# LBF_MANY_COM\n",
    "merged.loc[merged['LBF_MANY_COM_df'] == 1, \"LBF_MANY_COM\"] = 1\n",
    "merged.loc[(merged[\"LBF_MANY_COM_df\"] == 2), \"LBF_MANY_COM\"] = 0\n",
    "merged.loc[merged[\"LBF_MANY_COM_df\"].isin([pd.NaT,8,9]), \"LBF_MANY_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"LBF_MANY_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"LBF_MANY_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 13869\n",
      "Number of missing values after re-encoding: 13815\n"
     ]
    }
   ],
   "source": [
    "# LBF_CURR_COM\n",
    "merged.loc[merged['LBF_CURR_COM_df'] == 1, \"LBF_CURR_COM\"] = 1\n",
    "merged.loc[(merged[\"LBF_CURR_COM_df\"] == 2) | (merged[\"LBF_EVER_COM_df\"] == 2), \"LBF_CURR_COM\"] = 0\n",
    "merged.loc[merged[\"LBF_CURR_COM_df\"].isin([pd.NaT,8,9]), \"LBF_CURR_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"LBF_CURR_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"LBF_CURR_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 13810\n",
      "Number of missing values after re-encoding: 13813\n"
     ]
    }
   ],
   "source": [
    "# LBF_EVER_COM\n",
    "merged.loc[merged['LBF_EVER_COM_df'] == 1, \"LBF_EVER_COM\"] = 1\n",
    "merged.loc[(merged[\"LBF_EVER_COM_df\"] == 2), \"LBF_EVER_COM\"] = 0\n",
    "merged.loc[merged[\"LBF_EVER_COM_df\"].isin([pd.NaT,8,9]), \"LBF_EVER_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"LBF_EVER_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"LBF_EVER_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases before re-encoding: 3909\n",
      "Number of cases after re-encoding: 3909\n",
      "Number of missing values before re-encoding: 13089\n",
      "Number of missing values after re-encoding: 13135\n"
     ]
    }
   ],
   "source": [
    "# SMK_WHLCG_COM\n",
    "merged.loc[merged['SMK_WHLCG_COM_df'] == 1 | (merged[\"SMK_100CG_COM_df\"] == 1), \"SMK_WHLCG_COM\"] = 1\n",
    "merged.loc[(merged[\"SMK_WHLCG_COM_df\"] == 2) , \"SMK_WHLCG_COM\"] = 0\n",
    "merged.loc[merged[\"SMK_WHLCG_COM_df\"].isin([pd.NaT,8,9]), \"SMK_WHLCG_COM\"] = pd.NaT\n",
    "\n",
    "print(f'Number of cases before re-encoding: {merged[\"SMK_WHLCG_COM_df\"].eq(1).sum()}')\n",
    "print(f'Number of cases after re-encoding: {merged[\"SMK_WHLCG_COM\"].eq(1).sum()}')\n",
    "\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SMK_WHLCG_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SMK_WHLCG_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 12775\n",
      "Number of missing values after re-encoding: 2970\n"
     ]
    }
   ],
   "source": [
    "# INT_WYSSCL_OT_MCQ\n",
    "merged.loc[merged['INT_WYSSCL_OT_MCQ_df'] == 1, \"INT_WYSSCL_OT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"INT_WYSSCL_OT_MCQ_df\"] == 0) | (merged[\"INT_SCLNTWRK_MCQ_df\"] == 2), \"INT_WYSSCL_OT_MCQ\"] = 0\n",
    "merged.loc[merged[\"INT_WYSSCL_OT_MCQ_df\"].isin([pd.NaT,8,9]), \"INT_WYSSCL_OT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"INT_WYSSCL_OT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"INT_WYSSCL_OT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 11653\n",
      "Number of missing values after re-encoding: 13\n"
     ]
    }
   ],
   "source": [
    "# ICQ_FXCOLLR_COM\n",
    "merged.loc[merged['ICQ_FXCOLLR_COM_df'] == 1, \"ICQ_FXCOLLR_COM\"] = 1\n",
    "merged.loc[(merged[\"ICQ_FXCOLLR_COM_df\"] == 2) | (merged[\"ICQ_FX_COM_df\"] == 2), \"ICQ_FXCOLLR_COM\"] = 0\n",
    "merged.loc[merged[\"ICQ_FXCOLLR_COM_df\"].isin([pd.NaT,8,9]), \"ICQ_FXCOLLR_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ICQ_FXCOLLR_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ICQ_FXCOLLR_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 11653\n",
      "Number of missing values after re-encoding: 9\n"
     ]
    }
   ],
   "source": [
    "# ICQ_FXNOSE_COM\n",
    "merged.loc[merged['ICQ_FXNOSE_COM_df'] == 1, \"ICQ_FXNOSE_COM\"] = 1\n",
    "merged.loc[(merged[\"ICQ_FXNOSE_COM_df\"] == 2) | (merged[\"ICQ_FX_COM_df\"] == 2), \"ICQ_FXNOSE_COM\"] = 0\n",
    "merged.loc[merged[\"ICQ_FXNOSE_COM_df\"].isin([pd.NaT,8,9]), \"ICQ_FXNOSE_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ICQ_FXNOSE_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ICQ_FXNOSE_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 10783\n",
      "Number of missing values after re-encoding: 10811\n"
     ]
    }
   ],
   "source": [
    "# LFP_LNGST_COM\n",
    "merged.loc[merged['LFP_LNGST_COM_df'] == 1, \"LFP_LNGST_COM\"] = 1\n",
    "merged.loc[(merged[\"LFP_LNGST_COM_df\"] == 2) , \"LFP_LNGST_COM\"] = 0\n",
    "merged.loc[merged[\"LFP_LNGST_COM_df\"].isin([pd.NaT,8,9]), \"LFP_LNGST_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"LFP_LNGST_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"LFP_LNGST_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 9267\n",
      "Number of missing values after re-encoding: 9280\n"
     ]
    }
   ],
   "source": [
    "# SMK_EVRDL_COM\n",
    "merged.loc[merged['SMK_EVRDL_COM_df'] == 1, \"SMK_EVRDL_COM\"] = 1\n",
    "merged.loc[(merged[\"SMK_EVRDL_COM_df\"] == 2) , \"SMK_EVRDL_COM\"] = 0\n",
    "merged.loc[merged[\"SMK_EVRDL_COM_df\"].isin([pd.NaT,8,9]), \"SMK_EVRDL_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SMK_EVRDL_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SMK_EVRDL_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 7569\n",
      "Number of missing values after re-encoding: 7601\n"
     ]
    }
   ],
   "source": [
    "# RET_SPSE_COM\n",
    "merged.loc[merged['RET_SPSE_COM_df'] == 1, \"RET_SPSE_COM\"] = 1\n",
    "merged.loc[(merged[\"RET_SPSE_COM_df\"] == 2) , \"RET_SPSE_COM\"] = 0\n",
    "merged.loc[merged[\"RET_SPSE_COM_df\"].isin([pd.NaT,8,9]), \"RET_SPSE_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"RET_SPSE_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"RET_SPSE_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 3051\n"
     ]
    }
   ],
   "source": [
    "# SNO_SNORE_MCQ\n",
    "merged.loc[merged['SNO_SNORE_MCQ_df'] == 1, \"SNO_SNORE_MCQ\"] = 1\n",
    "merged.loc[(merged[\"SNO_SNORE_MCQ_df\"] == 2) , \"SNO_SNORE_MCQ\"] = 0\n",
    "merged.loc[merged[\"SNO_SNORE_MCQ_df\"].isin([pd.NaT,8,9]), \"SNO_SNORE_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SNO_SNORE_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SNO_SNORE_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 2923\n",
      "Number of missing values after re-encoding: 2970\n"
     ]
    }
   ],
   "source": [
    "# INT_SCLNTWRK_MCQ\n",
    "merged.loc[merged['INT_SCLNTWRK_MCQ_df'] == 1, \"INT_SCLNTWRK_MCQ\"] = 1\n",
    "merged.loc[(merged[\"INT_SCLNTWRK_MCQ_df\"] == 2) , \"INT_SCLNTWRK_MCQ\"] = 0\n",
    "merged.loc[merged[\"INT_SCLNTWRK_MCQ_df\"].isin([pd.NaT,8,9]), \"INT_SCLNTWRK_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"INT_SCLNTWRK_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"INT_SCLNTWRK_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1485\n",
      "Number of missing values after re-encoding: 1792\n"
     ]
    }
   ],
   "source": [
    "# IHD_ANGIO_COM\n",
    "merged.loc[merged['IHD_ANGIO_COM_df'] == 1, \"IHD_ANGIO_COM\"] = 1\n",
    "merged.loc[(merged[\"IHD_ANGIO_COM_df\"] == 2) , \"IHD_ANGIO_COM\"] = 0\n",
    "merged.loc[merged[\"IHD_ANGIO_COM_df\"].isin([pd.NaT,8,9]), \"IHD_ANGIO_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"IHD_ANGIO_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"IHD_ANGIO_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1648\n",
      "Number of missing values after re-encoding: 31\n"
     ]
    }
   ],
   "source": [
    "# ED_HSGR_COM\n",
    "merged.loc[merged['ED_HSGR_COM_df'] == 1, \"ED_HSGR_COM\"] = 1\n",
    "merged.loc[(merged[\"ED_HSGR_COM_df\"] == 2) | (merged[\"ED_ELHS_COM_df\"] == 2) | (merged[\"ED_ELHS_COM_df\"] == 1), \"ED_HSGR_COM\"] = 0\n",
    "merged.loc[merged[\"ED_HSGR_COM_df\"].isin([pd.NaT,8,9]), \"ED_HSGR_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ED_HSGR_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ED_HSGR_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 1485\n",
      "Number of missing values after re-encoding: 1550\n"
     ]
    }
   ],
   "source": [
    "# ROS_PAIN_COM\n",
    "merged.loc[merged['ROS_PAIN_COM_df'] == 1, \"ROS_PAIN_COM\"] = 1\n",
    "merged.loc[(merged[\"ROS_PAIN_COM_df\"] == 2), \"ROS_PAIN_COM\"] = 0\n",
    "merged.loc[merged[\"ROS_PAIN_COM_df\"].isin([pd.NaT,8,9]), \"ROS_PAIN_COM\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ROS_PAIN_COM_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ROS_PAIN_COM\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 1422\n"
     ]
    }
   ],
   "source": [
    "# SNO_STOPBREATH_MCQ\n",
    "merged.loc[merged['SNO_STOPBREATH_MCQ_df'] == 1, \"SNO_STOPBREATH_MCQ\"] = 1\n",
    "merged.loc[(merged[\"SNO_STOPBREATH_MCQ_df\"] == 2), \"SNO_STOPBREATH_MCQ\"] = 0\n",
    "merged.loc[merged[\"SNO_STOPBREATH_MCQ_df\"].isin([pd.NaT,8,9]), \"SNO_STOPBREATH_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"SNO_STOPBREATH_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"SNO_STOPBREATH_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 1191\n"
     ]
    }
   ],
   "source": [
    "# WEA_LFINS_MCQ\n",
    "merged.loc[merged['WEA_LFINS_MCQ_df'] == 1, \"WEA_LFINS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"WEA_LFINS_MCQ_df\"] == 2), \"WEA_LFINS_MCQ\"] = 0\n",
    "merged.loc[merged[\"WEA_LFINS_MCQ_df\"].isin([pd.NaT,8,9]), \"WEA_LFINS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"WEA_LFINS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"WEA_LFINS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 1181\n"
     ]
    }
   ],
   "source": [
    "# NUR_DHNR_MCQ\n",
    "merged.loc[merged['NUR_DHNR_MCQ_df'] == 1, \"NUR_DHNR_MCQ\"] = 1\n",
    "merged.loc[(merged[\"NUR_DHNR_MCQ_df\"] == 0), \"NUR_DHNR_MCQ\"] = 0\n",
    "merged.loc[merged[\"NUR_DHNR_MCQ_df\"].isin([pd.NaT,8,9]), \"NUR_DHNR_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"NUR_DHNR_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"NUR_DHNR_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 1026\n"
     ]
    }
   ],
   "source": [
    "# DSU_VITB12_MCQ\n",
    "merged.loc[merged['DSU_VITB12_MCQ_df'] == 1, \"DSU_VITB12_MCQ\"] = 1\n",
    "merged.loc[(merged[\"DSU_VITB12_MCQ_df\"] == 2), \"DSU_VITB12_MCQ\"] = 0\n",
    "merged.loc[merged[\"DSU_VITB12_MCQ_df\"].isin([pd.NaT,8,9]), \"DSU_VITB12_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"DSU_VITB12_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"DSU_VITB12_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 1009\n"
     ]
    }
   ],
   "source": [
    "# DSU_IRON_MCQ\n",
    "merged.loc[merged['DSU_IRON_MCQ_df'] == 1, \"DSU_IRON_MCQ\"] = 1\n",
    "merged.loc[(merged[\"DSU_IRON_MCQ_df\"] == 2), \"DSU_IRON_MCQ\"] = 0\n",
    "merged.loc[merged[\"DSU_IRON_MCQ_df\"].isin([pd.NaT,8,9]), \"DSU_IRON_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"DSU_IRON_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"DSU_IRON_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 1006\n"
     ]
    }
   ],
   "source": [
    "# DSU_VITC_MCQ\n",
    "merged.loc[merged['DSU_VITC_MCQ_df'] == 1, \"DSU_VITC_MCQ\"] = 1\n",
    "merged.loc[(merged[\"DSU_VITC_MCQ_df\"] == 2), \"DSU_VITC_MCQ\"] = 0\n",
    "merged.loc[merged[\"DSU_VITC_MCQ_df\"].isin([pd.NaT,8,9]), \"DSU_VITC_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"DSU_VITC_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"DSU_VITC_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 1000\n"
     ]
    }
   ],
   "source": [
    "# DSU_VITD_MCQ\n",
    "merged.loc[merged['DSU_VITD_MCQ_df'] == 1, \"DSU_VITD_MCQ\"] = 1\n",
    "merged.loc[(merged[\"DSU_VITD_MCQ_df\"] == 2), \"DSU_VITD_MCQ\"] = 0\n",
    "merged.loc[merged[\"DSU_VITD_MCQ_df\"].isin([pd.NaT,8,9]), \"DSU_VITD_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"DSU_VITD_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"DSU_VITD_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 996\n"
     ]
    }
   ],
   "source": [
    "# DSU_CAL_MCQ\n",
    "merged.loc[merged['DSU_CAL_MCQ_df'] == 1, \"DSU_CAL_MCQ\"] = 1\n",
    "merged.loc[(merged[\"DSU_CAL_MCQ_df\"] == 2), \"DSU_CAL_MCQ\"] = 0\n",
    "merged.loc[merged[\"DSU_CAL_MCQ_df\"].isin([pd.NaT,8,9]), \"DSU_CAL_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"DSU_CAL_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"DSU_CAL_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 982\n"
     ]
    }
   ],
   "source": [
    "# DSU_MLTV_MCQ\n",
    "merged.loc[merged['DSU_MLTV_MCQ_df'] == 1, \"DSU_MLTV_MCQ\"] = 1\n",
    "merged.loc[(merged[\"DSU_MLTV_MCQ_df\"] == 2), \"DSU_MLTV_MCQ\"] = 0\n",
    "merged.loc[merged[\"DSU_MLTV_MCQ_df\"].isin([pd.NaT,8,9]), \"DSU_MLTV_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"DSU_MLTV_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"DSU_MLTV_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 1013\n"
     ]
    }
   ],
   "source": [
    "# NUR_CKMEALS_MCQ\n",
    "merged.loc[merged['NUR_CKMEALS_MCQ_df'] == 1, \"NUR_CKMEALS_MCQ\"] = 1\n",
    "merged.loc[(merged[\"NUR_CKMEALS_MCQ_df\"] == 2), \"NUR_CKMEALS_MCQ\"] = 0\n",
    "merged.loc[merged[\"NUR_CKMEALS_MCQ_df\"].isin([pd.NaT,8,9]), \"NUR_CKMEALS_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"NUR_CKMEALS_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"NUR_CKMEALS_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 1007\n"
     ]
    }
   ],
   "source": [
    "# PA2_PARTPA_MCQ\n",
    "merged.loc[merged['PA2_PARTPA_MCQ_df'] == 1, \"PA2_PARTPA_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_PARTPA_MCQ_df\"] == 2), \"PA2_PARTPA_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_PARTPA_MCQ_df\"].isin([pd.NaT,8,9]), \"PA2_PARTPA_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_PARTPA_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_PARTPA_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 998\n"
     ]
    }
   ],
   "source": [
    "# HCU_OPTO_MCQ\n",
    "merged.loc[merged['HCU_OPTO_MCQ_df'] == 1, \"HCU_OPTO_MCQ\"] = 1\n",
    "merged.loc[(merged[\"HCU_OPTO_MCQ_df\"] == 2), \"HCU_OPTO_MCQ\"] = 0\n",
    "merged.loc[merged[\"HCU_OPTO_MCQ_df\"].isin([pd.NaT,8,9]), \"HCU_OPTO_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"HCU_OPTO_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"HCU_OPTO_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 987\n"
     ]
    }
   ],
   "source": [
    "# HCU_SPEC_MCQ\n",
    "merged.loc[merged['HCU_SPEC_MCQ_df'] == 1, \"HCU_SPEC_MCQ\"] = 1\n",
    "merged.loc[(merged[\"HCU_SPEC_MCQ_df\"] == 2), \"HCU_SPEC_MCQ\"] = 0\n",
    "merged.loc[merged[\"HCU_SPEC_MCQ_df\"].isin([pd.NaT,8,9]), \"HCU_SPEC_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"HCU_SPEC_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"HCU_SPEC_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 978\n"
     ]
    }
   ],
   "source": [
    "# HCU_PHYSIO_MCQ\n",
    "merged.loc[merged['HCU_PHYSIO_MCQ_df'] == 1, \"HCU_PHYSIO_MCQ\"] = 1\n",
    "merged.loc[(merged[\"HCU_PHYSIO_MCQ_df\"] == 2), \"HCU_PHYSIO_MCQ\"] = 0\n",
    "merged.loc[merged[\"HCU_PHYSIO_MCQ_df\"].isin([pd.NaT,8,9]), \"HCU_PHYSIO_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"HCU_PHYSIO_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"HCU_PHYSIO_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 979\n"
     ]
    }
   ],
   "source": [
    "# HCU_FAMPHY_MCQ\n",
    "merged.loc[merged['HCU_FAMPHY_MCQ_df'] == 1, \"HCU_FAMPHY_MCQ\"] = 1\n",
    "merged.loc[(merged[\"HCU_FAMPHY_MCQ_df\"] == 2), \"HCU_FAMPHY_MCQ\"] = 0\n",
    "merged.loc[merged[\"HCU_FAMPHY_MCQ_df\"].isin([pd.NaT,8,9]), \"HCU_FAMPHY_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"HCU_FAMPHY_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"HCU_FAMPHY_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 983\n"
     ]
    }
   ],
   "source": [
    "# HCU_EMEREG_MCQ\n",
    "merged.loc[merged['HCU_EMEREG_MCQ_df'] == 1, \"HCU_EMEREG_MCQ\"] = 1\n",
    "merged.loc[(merged[\"HCU_EMEREG_MCQ_df\"] == 2), \"HCU_EMEREG_MCQ\"] = 0\n",
    "merged.loc[merged[\"HCU_EMEREG_MCQ_df\"].isin([pd.NaT,8,9]), \"HCU_EMEREG_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"HCU_EMEREG_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"HCU_EMEREG_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 974\n"
     ]
    }
   ],
   "source": [
    "# HCU_PSYCH_MCQ\n",
    "merged.loc[merged['HCU_PSYCH_MCQ_df'] == 1, \"HCU_PSYCH_MCQ\"] = 1\n",
    "merged.loc[(merged[\"HCU_PSYCH_MCQ_df\"] == 2), \"HCU_PSYCH_MCQ\"] = 0\n",
    "merged.loc[merged[\"HCU_PSYCH_MCQ_df\"].isin([pd.NaT,8,9]), \"HCU_PSYCH_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"HCU_PSYCH_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"HCU_PSYCH_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 963\n"
     ]
    }
   ],
   "source": [
    "# HCU_DEN_MCQ\n",
    "merged.loc[merged['HCU_DEN_MCQ_df'] == 1, \"HCU_DEN_MCQ\"] = 1\n",
    "merged.loc[(merged[\"HCU_DEN_MCQ_df\"] == 2), \"HCU_DEN_MCQ\"] = 0\n",
    "merged.loc[merged[\"HCU_DEN_MCQ_df\"].isin([pd.NaT,8,9]), \"HCU_DEN_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"HCU_DEN_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"HCU_DEN_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 957\n"
     ]
    }
   ],
   "source": [
    "# HCU_HLOVRNT_MCQ\n",
    "merged.loc[merged['HCU_HLOVRNT_MCQ_df'] == 1, \"HCU_HLOVRNT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"HCU_HLOVRNT_MCQ_df\"] == 2), \"HCU_HLOVRNT_MCQ\"] = 0\n",
    "merged.loc[merged[\"HCU_HLOVRNT_MCQ_df\"].isin([pd.NaT,8,9]), \"HCU_HLOVRNT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"HCU_HLOVRNT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"HCU_HLOVRNT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 980\n"
     ]
    }
   ],
   "source": [
    "# INT_ACCESSHM_MCQ\n",
    "merged.loc[merged['INT_ACCESSHM_MCQ_df'] == 1, \"INT_ACCESSHM_MCQ\"] = 1\n",
    "merged.loc[(merged[\"INT_ACCESSHM_MCQ_df\"] == 2), \"INT_ACCESSHM_MCQ\"] = 0\n",
    "merged.loc[merged[\"INT_ACCESSHM_MCQ_df\"].isin([pd.NaT,8,9]), \"INT_ACCESSHM_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"INT_ACCESSHM_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"INT_ACCESSHM_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 973\n"
     ]
    }
   ],
   "source": [
    "# HUP_FREE_MCQ\n",
    "merged.loc[merged['HUP_FREE_MCQ_df'] == 1, \"HUP_FREE_MCQ\"] = 1\n",
    "merged.loc[(merged[\"HUP_FREE_MCQ_df\"] == 2), \"HUP_FREE_MCQ\"] = 0\n",
    "merged.loc[merged[\"HUP_FREE_MCQ_df\"].isin([pd.NaT,8,9]), \"HUP_FREE_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"HUP_FREE_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"HUP_FREE_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 964\n"
     ]
    }
   ],
   "source": [
    "# PA2_HMREPAIR_MCQ\n",
    "merged.loc[merged['PA2_HMREPAIR_MCQ_df'] == 1, \"PA2_HMREPAIR_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_HMREPAIR_MCQ_df\"] == 2), \"PA2_HMREPAIR_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_HMREPAIR_MCQ_df\"].isin([pd.NaT,8,9]), \"PA2_HMREPAIR_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_HMREPAIR_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_HMREPAIR_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 957\n"
     ]
    }
   ],
   "source": [
    "# PA2_CRPRSN_MCQ\n",
    "merged.loc[merged['PA2_CRPRSN_MCQ_df'] == 1, \"PA2_CRPRSN_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_CRPRSN_MCQ_df\"] == 2), \"PA2_CRPRSN_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_CRPRSN_MCQ_df\"].isin([pd.NaT,8,9]), \"PA2_CRPRSN_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_CRPRSN_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_CRPRSN_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 958\n"
     ]
    }
   ],
   "source": [
    "# PA2_HVYHSWK_MCQ\n",
    "merged.loc[merged['PA2_HVYHSWK_MCQ_df'] == 1, \"PA2_HVYHSWK_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_HVYHSWK_MCQ_df\"] == 2), \"PA2_HVYHSWK_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_HVYHSWK_MCQ_df\"].isin([pd.NaT,8,9]), \"PA2_HVYHSWK_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_HVYHSWK_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_HVYHSWK_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 957\n"
     ]
    }
   ],
   "source": [
    "# PA2_LTHSWK_MCQ\n",
    "merged.loc[merged['PA2_LTHSWK_MCQ_df'] == 1, \"PA2_LTHSWK_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_LTHSWK_MCQ_df\"] == 2), \"PA2_LTHSWK_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_LTHSWK_MCQ_df\"].isin([pd.NaT,8,9]), \"PA2_LTHSWK_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_LTHSWK_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_LTHSWK_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 954\n"
     ]
    }
   ],
   "source": [
    "# PA2_WRK_MCQ\n",
    "merged.loc[merged['PA2_WRK_MCQ_df'] == 1, \"PA2_WRK_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_WRK_MCQ_df\"] == 2), \"PA2_WRK_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_WRK_MCQ_df\"].isin([pd.NaT,8,9]), \"PA2_WRK_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_WRK_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_WRK_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 954\n"
     ]
    }
   ],
   "source": [
    "# PA2_HVYODA_MCQ\n",
    "merged.loc[merged['PA2_HVYODA_MCQ_df'] == 1, \"PA2_HVYODA_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_HVYODA_MCQ_df\"] == 2), \"PA2_HVYODA_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_HVYODA_MCQ_df\"].isin([pd.NaT,8,9]), \"PA2_HVYODA_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_HVYODA_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_HVYODA_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 951\n"
     ]
    }
   ],
   "source": [
    "# PA2_LTODA_MCQ\n",
    "merged.loc[merged['PA2_LTODA_MCQ_df'] == 1, \"PA2_LTODA_MCQ\"] = 1\n",
    "merged.loc[(merged[\"PA2_LTODA_MCQ_df\"] == 2), \"PA2_LTODA_MCQ\"] = 0\n",
    "merged.loc[merged[\"PA2_LTODA_MCQ_df\"].isin([pd.NaT,8,9]), \"PA2_LTODA_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"PA2_LTODA_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"PA2_LTODA_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 959\n"
     ]
    }
   ],
   "source": [
    "# FAL_12MN_MCQ\n",
    "merged.loc[merged['FAL_12MN_MCQ_df'] == 1, \"FAL_12MN_MCQ\"] = 1\n",
    "merged.loc[(merged[\"FAL_12MN_MCQ_df\"] == 2), \"FAL_12MN_MCQ\"] = 0\n",
    "merged.loc[merged[\"FAL_12MN_MCQ_df\"].isin([pd.NaT,8,9]), \"FAL_12MN_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"FAL_12MN_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"FAL_12MN_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 951\n"
     ]
    }
   ],
   "source": [
    "# ORH_DENT_MCQ\n",
    "merged.loc[merged['ORH_DENT_MCQ_df'] == 1, \"ORH_DENT_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_DENT_MCQ_df\"] == 2), \"ORH_DENT_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_DENT_MCQ_df\"].isin([pd.NaT,8,9]), \"ORH_DENT_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_DENT_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_DENT_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before re-encoding: 947\n",
      "Number of missing values after re-encoding: 948\n"
     ]
    }
   ],
   "source": [
    "# ORH_TEETH_MCQ\n",
    "merged.loc[merged['ORH_TEETH_MCQ_df'] == 1, \"ORH_TEETH_MCQ\"] = 1\n",
    "merged.loc[(merged[\"ORH_TEETH_MCQ_df\"] == 2), \"ORH_TEETH_MCQ\"] = 0\n",
    "merged.loc[merged[\"ORH_TEETH_MCQ_df\"].isin([pd.NaT,8,9]), \"ORH_TEETH_MCQ\"] = pd.NaT\n",
    "print(f'Number of missing values before re-encoding: {merged[\"ORH_TEETH_MCQ_df\"].isna().sum()}')\n",
    "print(f'Number of missing values after re-encoding: {merged[\"ORH_TEETH_MCQ\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Special case: **Diabetes**; we break it down into two new phenotypes (type1 and type2)\n",
    "THIS IS WRONG, REFER TO /lustre06/project/6061810/mikekaz/CLSA/Regenie/old_files/data/diabetes_only.ipynb FOR THE CORRECT VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with Type1 Diabetes: 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2282412/1368668307.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged.loc[merged['DIA_TYPE_COM_df'] == 1, \"DIA_TYPE1_COM\"] = 1\n"
     ]
    }
   ],
   "source": [
    "# # DIA_TYPE1_COM\n",
    "# merged.loc[merged['DIA_TYPE_COM_df'] == 1, \"DIA_TYPE1_COM\"] = 1\n",
    "# merged.loc[merged[\"DIA_TYPE_COM_df\"].isin([2,3]), \"DIA_TYPE1_COM\"] = 0\n",
    "# merged.loc[merged[\"DIA_TYPE_COM_df\"].isin([pd.NaT,8,9]), \"DIA_TYPE1_COM\"] = pd.NaT\n",
    "# print(f\"Number of people with Type1 Diabetes: {(merged['DIA_TYPE1_COM'] == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with Type1 Diabetes: 2130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2282412/3922268862.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged.loc[merged['DIA_TYPE_COM_df'] == 2, \"DIA_TYPE2_COM\"] = 1\n"
     ]
    }
   ],
   "source": [
    "# # DIA_TYPE2_COM\n",
    "# merged.loc[merged['DIA_TYPE_COM_df'] == 2, \"DIA_TYPE2_COM\"] = 1\n",
    "# merged.loc[merged[\"DIA_TYPE_COM_df\"].isin([1,3]), \"DIA_TYPE2_COM\"] = 0\n",
    "# merged.loc[merged[\"DIA_TYPE_COM_df\"].isin([pd.NaT,8,9]), \"DIA_TYPE2_COM\"] = pd.NaT\n",
    "# print(f\"Number of people with Type1 Diabetes: {(merged['DIA_TYPE2_COM'] == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not nested phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not nested, [case:1,control:2]\n",
    "x12 = [\"PKD_FACE_COM\",\"OST_MOM_COM\",\"OST_CST_COM\",\"PKD_SMWRT_COM\",\"TMT_PASS_COM\",\"CCC_ARTOT_COM\",\"CCC_UTHYR_COM\",\"CCC_OAKNEE_COM\",\"CCC_OAHIP_COM\",\"CCC_OAHAND_COM\",\"CCC_OSTPO_COM\",\"CCC_ALLRG_COM\",\"CAO_SOBPM_COM\",\"FAS_COMP_F_COM\",\"FAS_COMP_A_COM\",\"FAS_COMP_S_COM\",\"CCC_MACDEG_COM\",\"CAO_SOBUP_COM\",\"CCC_F2_COM\",\"OSK_PAINSL_COM\",\"HBP_TRT_COM\",\"OSK_SWELL_COM\",\"CCC_COPD_COM\",\"CCC_HEART_COM\",\"CCC_HBP_COM\",\"PKD_VOICE_COM\",\"RET_RTRN_COM\",\"CCC_F1_COM\",\"CCC_PVD_COM\",\"CCC_ANGI_COM\",\"PKD_BAL_COM\",\"OSH_PAINSL_COM\",\"PKD_WALK_COM\",\"DPR_CLINDEP_COM\",\"CAO_EXERT_COM\",\"CCC_AMI_COM\",\"CCC_ULCR_COM\",\"CAO_COLD_COM\",\"CAO_SOBFLAT_COM\",\"OSA_LGTMB_COM\",\"CCC_ASTHM_COM\",\"CCC_IBDIBS_COM\",\"OST_BONE_COM\",\"CAO_WKCOF_COM\",\"OSH_LOM_COM\",\"CAO_WHEZ_COM\",\"CAO_COFPY_COM\",\"STR_WEAK_COM\",\"CCC_ANXI_COM\",\"CCC_BCKP_COM\",\"PKD_RISE_COM\",\"OST_BP_COM\",\"STR_NOVIS_COM\",\"CCC_DRFLU_COM\",\"OSA_LGJNT_COM\",\"DIA_DIAB_COM\",\"CCC_MOOD_COM\",\"CCC_MGRN_COM\",\"STR_VIS_COM\",\"CCC_DRUTI_COM\",\"STR_NOEXP_COM\",\"CCC_URIINC_COM\",\"OSK_PAIN_COM\",\"STR_NUMB_COM\",\"CAO_WKWHEZ_COM\",\"OSA_PAINTMB_COM\",\"CCC_CANC_COM\",\"IHD_CAB_COM\",\"OSA_PAINJNT_COM\",\"PKD_BUTON_COM\",\"OSH_PAIN_COM\",\"PKD_SHKE_COM\",\"SPA_MORAC_COM\",\"OSK_KNERPL_COM\",\"SPA_FPAR_COM\",\"INJ_OCC_COM\",\"SMK_100CG_COM\",\"CAG_FPAS_COM\",\"CR1_FRHC_COM\",\"CR2_FRHC_COM\",\"ED_OTED_COM\",\"SMK_OTREG_COM\",\"SDC_FIMM_COM\",\"ALC_EVER_COM\",\"TBI_POSITIVE_COM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not nested, [case:1,control:0]\n",
    "x10 = [\"WLK_SMPTS_PAIN_COM\",\"WLK_SMPTS_NONE_COM\",\"PSD_DCTOFF_COM\",\"TBI_TYP_VH_COM\",\"TBI_TYP_FL_COM\",\"TBI_TYP_SPT_COM\",\"TBI_TYP_NONE_COM\",\"SPA_SOAC_RNP_COM\",\"SPA_SOAC_HY_COM\",\"SPA_SOAC_HIC_COM\",\"SPA_SOAC_HOC_COM\",\"SPA_SOAC_DT_COM\",\"SPA_SOAC_INT_COM\",\"SPA_SOAC_VOT_COM\",\"CR1_PRO_AC_COM\",\"CR1_PRO_NONE_COM\",\"CR2_FAM_AC_COM\",\"CR2_FAM_TR_COM\",\"CR2_FAM_ML_COM\",\"CR2_FAM_NONE_COM\",\"CR2_DEVC_CN_COM\",\"CR2_DEVC_BR_COM\",\"CR2_DEVC_BT_COM\",\"CR2_DEVC_NONE_COM\",\"CAG_HLT_PR_COM\",\"CAG_HLT_MD_COM\",\"CAG_HLT_MG_COM\",\"CAG_HLT_AC_COM\",\"CAG_HLT_TR_COM\",\"CAG_HLT_ML_COM\",\"CAG_HLT_NONE_COM\",\"CAG_HLT_CS_COM\",\"INC_SRCE_WG_COM\",\"INC_SRCE_SE_COM\",\"INC_SRCE_IN_COM\",\"INC_SRCE_BN_COM\",\n",
    "\"INC_SRCE_PN_COM\",\"INC_SRCE_GV_COM\",\"INC_SRCE_OLD_COM\",\"INC_SRCE_GIS_COM\",\"INC_SRCE_CH_COM\",\"INC_SRCE_CP_COM\",\"INC_PSRCE_WG_COM\",\"INC_PSRCE_SE_COM\",\"INC_PSRCE_IN_COM\",\"INC_PSRCE_BN_COM\",\"INC_PSRCE_PN_COM\",\"INC_PSRCE_GV_COM\",\"INC_PSRCE_OLD_COM\",\"INC_PSRCE_CP_COM\",\"DEP_DPSFD_COM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in x12:\n",
    "    merged.loc[merged[f\"{var}_df\"] == 1, var] = 1\n",
    "\n",
    "    merged.loc[merged[f\"{var}_df\"] == 2, var] = 0\n",
    "\n",
    "    merged.loc[merged[f\"{var}_df\"].isin([8, 9, pd.NaT]), var] = pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in x10:\n",
    "    merged.loc[merged[f\"{var}_df\"] == 1, var] = 1\n",
    "\n",
    "    merged.loc[merged[f\"{var}_df\"] == 0, var] = 0\n",
    "\n",
    "    merged.loc[merged[f\"{var}_df\"].isin([8, 9, pd.NaT]), var] = pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24505, 4234)\n",
      "(24505, 352)\n"
     ]
    }
   ],
   "source": [
    "print(merged.shape)\n",
    "# Drop columns from 'merged' that are empty\n",
    "columns_with_nats = merged.columns[merged.isna().all()]\n",
    "merged = merged.drop(columns=columns_with_nats)\n",
    "\n",
    "# Drop columns from 'merged' with names ending in '_df'\n",
    "merged = merged.loc[:, ~merged.columns.str.endswith('_df')]\n",
    "\n",
    "# Drop \"ADM_GWAS3_COM\" from 'merged'\n",
    "merged = merged.drop([\"ADM_GWAS3_COM\"], axis=1)\n",
    "merged.head()\n",
    "print(merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24505, 352)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2282412/4065482639.py:17: FutureWarning: The default value of numeric_only in DataFrame.std is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  std_devs = merged.std()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>IID</th>\n",
       "      <th>ED_HSGR_COM</th>\n",
       "      <th>ED_OTED_COM</th>\n",
       "      <th>SMK_100CG_COM</th>\n",
       "      <th>SMK_WHLCG_COM</th>\n",
       "      <th>SMK_EVRDL_COM</th>\n",
       "      <th>SMK_OTREG_COM</th>\n",
       "      <th>SMK_TYPEOT_PI_COM</th>\n",
       "      <th>ALC_EVER_COM</th>\n",
       "      <th>...</th>\n",
       "      <th>PKD_RISE_COM</th>\n",
       "      <th>OST_MOM_COM</th>\n",
       "      <th>OST_CST_COM</th>\n",
       "      <th>OST_BP_COM</th>\n",
       "      <th>OST_BCKPPM_COM</th>\n",
       "      <th>ICQ_FXNOSE_COM</th>\n",
       "      <th>ICQ_FXCOLLR_COM</th>\n",
       "      <th>ICQ_SMOKE24H_COM</th>\n",
       "      <th>DIA_TYPE1_COM</th>\n",
       "      <th>DIA_TYPE2_COM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17358</td>\n",
       "      <td>17358</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19115</td>\n",
       "      <td>19115</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23576</td>\n",
       "      <td>23576</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10778</td>\n",
       "      <td>10778</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6013</td>\n",
       "      <td>6013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11060</td>\n",
       "      <td>11060</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22584</td>\n",
       "      <td>22584</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18121</td>\n",
       "      <td>18121</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9972</td>\n",
       "      <td>9972</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18543</td>\n",
       "      <td>18543</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17096</td>\n",
       "      <td>17096</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3972</td>\n",
       "      <td>3972</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8206</td>\n",
       "      <td>8206</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4733</td>\n",
       "      <td>4733</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4293</td>\n",
       "      <td>4293</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FID    IID ED_HSGR_COM ED_OTED_COM SMK_100CG_COM SMK_WHLCG_COM  \\\n",
       "0   17358  17358           0           1             1           NaT   \n",
       "1   19115  19115           1           0             1           NaT   \n",
       "2   23576  23576           1           1             0             0   \n",
       "3   10778  10778           1           1             1           NaT   \n",
       "4    6013   6013           1           1             0             0   \n",
       "5   11060  11060           1           1             0             1   \n",
       "6   22584  22584           1           1             0             1   \n",
       "7   18121  18121           1           1             0             1   \n",
       "8    9972   9972           1           1             1           NaT   \n",
       "9   18543  18543           1           1             0             0   \n",
       "10  17096  17096           1           1             1           NaT   \n",
       "11   3972   3972           1           1             1           NaT   \n",
       "12   8206   8206           1           1             0             1   \n",
       "13   4733   4733           1           1             1           NaT   \n",
       "14   4293   4293           1           1             0             1   \n",
       "\n",
       "   SMK_EVRDL_COM SMK_OTREG_COM SMK_TYPEOT_PI_COM ALC_EVER_COM  ...  \\\n",
       "0              1             0                 0            1  ...   \n",
       "1              1             1                 0            1  ...   \n",
       "2            NaT             0                 0            1  ...   \n",
       "3              1             0                 0            1  ...   \n",
       "4            NaT             0                 0            1  ...   \n",
       "5              0             0                 0            1  ...   \n",
       "6              0             0                 0            1  ...   \n",
       "7              0             1                 1            1  ...   \n",
       "8              1             0                 0            1  ...   \n",
       "9            NaT             0                 0            1  ...   \n",
       "10             1             1                 0            1  ...   \n",
       "11             1             0                 0            1  ...   \n",
       "12             0             0                 0            1  ...   \n",
       "13             0             0                 0            1  ...   \n",
       "14             0             0                 0            1  ...   \n",
       "\n",
       "   PKD_RISE_COM OST_MOM_COM OST_CST_COM OST_BP_COM  OST_BCKPPM_COM  \\\n",
       "0             0           0           0          1               0   \n",
       "1             0           1           0          1               1   \n",
       "2             0           0           0          0               0   \n",
       "3             1           0           0          0               0   \n",
       "4             0           0           0          0               0   \n",
       "5             1           0           1          1               0   \n",
       "6             0           1           0          0               0   \n",
       "7             0           0           0          0               0   \n",
       "8             0           0           1          0               0   \n",
       "9             0           1           0          0               0   \n",
       "10            0           1           0          0               0   \n",
       "11            0           0           0          1               1   \n",
       "12            0           0           1          0               0   \n",
       "13            0           0           0          0               0   \n",
       "14            0           0           0          1               1   \n",
       "\n",
       "    ICQ_FXNOSE_COM  ICQ_FXCOLLR_COM  ICQ_SMOKE24H_COM  DIA_TYPE1_COM  \\\n",
       "0                0                0                 0            NaN   \n",
       "1                0                0                 0              0   \n",
       "2                1                0                 0              0   \n",
       "3                0                0                 0              0   \n",
       "4                0                0                 0            NaN   \n",
       "5                0                0                 0            NaN   \n",
       "6                0                0                 0            NaN   \n",
       "7                0                0                 0              0   \n",
       "8                0                0                 0            NaN   \n",
       "9                0                0                 0            NaN   \n",
       "10               1                0                 0            NaN   \n",
       "11               0                0                 0              0   \n",
       "12               0                0                 0            NaN   \n",
       "13               1                0                 0            NaN   \n",
       "14               0                0                 0            NaN   \n",
       "\n",
       "    DIA_TYPE2_COM  \n",
       "0             NaN  \n",
       "1               1  \n",
       "2               0  \n",
       "3               1  \n",
       "4             NaN  \n",
       "5             NaN  \n",
       "6             NaN  \n",
       "7               0  \n",
       "8             NaN  \n",
       "9             NaN  \n",
       "10            NaN  \n",
       "11              1  \n",
       "12            NaN  \n",
       "13            NaN  \n",
       "14            NaN  \n",
       "\n",
       "[15 rows x 352 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def float_to_int(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    return int(x)\n",
    "\n",
    "merged = merged.applymap(float_to_int)\n",
    "\n",
    "# Modify the values in the FID and IID columns\n",
    "# merged['FID'] = merged['FID'].astype(str) + '_' + merged['FID'].astype(str)\n",
    "# merged['IID'] = merged['IID'].astype(str) + '_' + merged['IID'].astype(str)\n",
    "\n",
    "merged['FID'] = merged['FID'].astype(str)\n",
    "merged['IID'] = merged['IID'].astype(str)\n",
    "\n",
    "\n",
    "# Calculate the standard deviation of each column\n",
    "std_devs = merged.std()\n",
    "\n",
    "# Get a list of columns where standard deviation is zero\n",
    "cols_to_drop = std_devs[std_devs == 0].index\n",
    "\n",
    "# Drop these columns from the dataframe\n",
    "merged = merged.drop(cols_to_drop, axis=1)\n",
    "\n",
    "print(merged.shape)\n",
    "merged.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataframe back to a CSV file with NaN values replacing any missing values\n",
    "merged.to_csv('GWAS_PHENOTYPE_FILE_FINAL_Mar9.csv',sep=' ', index=False, na_rep='NA', float_format='%.0f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
