{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f40e6eff-5eba-4215-9bf7-bf7a84cf3fcf",
   "metadata": {},
   "source": [
    "## This code generates the JSON file for phenotypes required by PheWeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf734de-fc46-4a0a-9f10-f0fd39dfb140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2020/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2020/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/openpyxl-3.1.2+computecanada-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/et_xmlfile-1.1.0+computecanada-py3-none-any.whl\n",
      "\u001b[33mWARNING: Error parsing requirements for scipy: [Errno 2] No such file or directory: '/home/mikekaz/.local/lib/python3.10/site-packages/scipy-1.11.2+computecanada.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile openpyxl\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install openpyxl --no-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc72b31e-02c7-414e-873b-6a702886d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created at: /lustre06/project/6061810/mikekaz/CLSA/QC/ALL_binary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the input directory where the .txt.gz files are located\n",
    "input_directory = '/home/mikekaz/scratch/CLSA_Mike/Regenie/FINAL/Regenie_nextflow/Binary/results/step2/summary/Pheweb_ready_summary_files'  # Change this to your actual directory\n",
    "\n",
    "# The path for the CSV file you want to create\n",
    "csv_file_path = '/lustre06/project/6061810/mikekaz/CLSA/QC/ALL_binary.csv'  # Change this to where you want to save the CSV\n",
    "\n",
    "\n",
    "# Get a list of all .txt.gz files in the input directory\n",
    "txt_gz_files = [os.path.join(input_directory, f) for f in os.listdir(input_directory) if f.endswith('.txt.gz')]\n",
    "\n",
    "# Define the CSV file's columns\n",
    "columns = ['assoc_files', 'phenocode', 'phenostring', 'category', 'num_cases', 'num_controls', 'num_samples']\n",
    "\n",
    "# Populate the CSV, 'assoc_files' will be populated with the file path, and 'phenocode' with the base name\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "    for file_path in txt_gz_files:\n",
    "        # Extract base name without the extension and the directory path\n",
    "        base_name = os.path.basename(file_path)  # Gets the filename from the path\n",
    "        phenocode = os.path.splitext(base_name)[0]  # Removes the extension from the filename\n",
    "        phenocode = phenocode.replace('.txt', '')  # Ensures .txt is not part of the phenocode\n",
    "        \n",
    "        # Remove any leading underscores from the phenocode\n",
    "        phenocode = phenocode.lstrip('_')\n",
    "        \n",
    "        writer.writerow({'assoc_files': file_path, 'phenocode': phenocode})\n",
    "\n",
    "print(f\"CSV file has been created at: {csv_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae560ddc-1982-496f-aeba-739e6c62f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file has been saved to: /lustre06/project/6061810/mikekaz/CLSA/QC/ALL_binary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "excel_path = '2109005_McGill_DTaliun_Baseline_CoPv7-dictionary.xlsx'\n",
    "df_excel = pd.read_excel(excel_path)\n",
    "\n",
    "# Convert the Excel data into a dictionary for easy lookup\n",
    "# The keys are the values from the \"name\" column, and the values are tuples containing \"label\" and \"class\" values\n",
    "pheno_dict = {row['name']: (row['label'], row['class']) for index, row in df_excel.iterrows()}\n",
    "\n",
    "# Now read the CSV, update it with phenostring and category, and then write it back\n",
    "updated_csv_path = '/lustre06/project/6061810/mikekaz/CLSA/QC/ALL_binary.csv'  # Path for the updated CSV\n",
    "\n",
    "# Reading the original CSV\n",
    "df_csv = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Function to update phenostring and category based on the phenocode\n",
    "def update_row(row):\n",
    "    phenocode = row['phenocode']\n",
    "    if phenocode in pheno_dict:\n",
    "        row['phenostring'], row['category'] = pheno_dict[phenocode]\n",
    "    else:\n",
    "        row['phenostring'], row['category'] = None, None  # Or use some default/fallback value\n",
    "    return row\n",
    "\n",
    "# Apply the update function to each row\n",
    "df_updated = df_csv.apply(update_row, axis=1)\n",
    "\n",
    "# Write the updated DataFrame back to a new CSV\n",
    "df_updated.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV file has been saved to: {updated_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605b4539-c20e-4622-a62b-1844f076044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CSV file has been saved to: /lustre06/project/6061810/mikekaz/CLSA/QC/ALL_binary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the phenotypes file\n",
    "phenotypes_path = 'PHENOTYPES_FILE_Mar9.csv'\n",
    "df_phenotypes = pd.read_csv(phenotypes_path, sep=' ')\n",
    "\n",
    "# Prepare a dictionary to hold the counts for each phenocode\n",
    "phenocode_counts = {}\n",
    "\n",
    "# Iterate over each phenocode column (starting from the third column)\n",
    "for phenocode in df_phenotypes.columns[2:]:\n",
    "    num_cases = df_phenotypes[phenocode].value_counts().get(1, 0)\n",
    "    num_controls = df_phenotypes[phenocode].value_counts().get(0, 0)\n",
    "    num_samples = num_cases + num_controls\n",
    "    phenocode_counts[phenocode] = (num_cases, num_controls, num_samples)\n",
    "\n",
    "# Now read the existing CSV, update it, and then write it back\n",
    "final_csv_path = '/lustre06/project/6061810/mikekaz/CLSA/QC/ALL_binary.csv'  # Path for the final updated CSV\n",
    "\n",
    "# Reading the updated CSV\n",
    "df_csv = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Function to update num_cases, num_controls, and num_samples based on the phenocode\n",
    "def update_counts(row):\n",
    "    phenocode = row['phenocode']\n",
    "    if phenocode in phenocode_counts:\n",
    "        row['num_cases'], row['num_controls'], row['num_samples'] = phenocode_counts[phenocode]\n",
    "    else:\n",
    "        row['num_cases'], row['num_controls'], row['num_samples'] = None, None, None  # Or use some default/fallback value\n",
    "    return row\n",
    "\n",
    "# Apply the update function to each row\n",
    "df_final = df_csv.apply(update_counts, axis=1)\n",
    "\n",
    "# Write the final DataFrame back to a new CSV\n",
    "df_final.to_csv(final_csv_path, index=False)\n",
    "\n",
    "print(f\"Final CSV file has been saved to: {final_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7172ffa7-e32e-4ad2-8281-853326e26801",
   "metadata": {},
   "source": [
    "## we need to add TYPE1,2 DIABETES to this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4d98ba-2ca5-4038-907b-4d7af4d00726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created at: /lustre06/project/6061810/mikekaz/CLSA/QC/DIA_pheweb_input_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the input directory where the .txt.gz files are located\n",
    "input_directory = '/home/mikekaz/scratch/CLSA_Mike/Regenie/FINAL/Regenie_nextflow/Binary/Diabetes_only/results/step2/summary'  # Change this to your actual directory\n",
    "\n",
    "# The path for the CSV file you want to create\n",
    "csv_file_path = '/lustre06/project/6061810/mikekaz/CLSA/QC/DIA_pheweb_input_data.csv'  # Change this to where you want to save the CSV\n",
    "\n",
    "\n",
    "# Get a list of all .txt.gz files in the input directory\n",
    "txt_gz_files = [os.path.join(input_directory, f) for f in os.listdir(input_directory) if f.endswith('.txt.gz')]\n",
    "\n",
    "# Define the CSV file's columns\n",
    "columns = ['assoc_files', 'phenocode', 'phenostring', 'category', 'num_cases', 'num_controls', 'num_samples']\n",
    "\n",
    "# Populate the CSV, 'assoc_files' will be populated with the file path, and 'phenocode' with the base name\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "    for file_path in txt_gz_files:\n",
    "        # Extract base name without the extension and the directory path\n",
    "        base_name = os.path.basename(file_path)  # Gets the filename from the path\n",
    "        phenocode = os.path.splitext(base_name)[0]  # Removes the extension from the filename\n",
    "        phenocode = phenocode.replace('.txt', '')  # Ensures .txt is not part of the phenocode\n",
    "        \n",
    "        # Remove any leading underscores from the phenocode\n",
    "        phenocode = phenocode.lstrip('_')\n",
    "        \n",
    "        writer.writerow({'assoc_files': file_path, 'phenocode': phenocode})\n",
    "\n",
    "print(f\"CSV file has been created at: {csv_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada305cd-6c97-4fc6-bbf5-e6ee5bec4c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file has been saved to: /lustre06/project/6061810/mikekaz/CLSA/QC/DIA_pheweb_input_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "excel_path = '2109005_McGill_DTaliun_Baseline_CoPv7-dictionary.xlsx'\n",
    "df_excel = pd.read_excel(excel_path)\n",
    "\n",
    "# Convert the Excel data into a dictionary for easy lookup\n",
    "# The keys are the values from the \"name\" column, and the values are tuples containing \"label\" and \"class\" values\n",
    "pheno_dict = {row['name']: (row['label'], row['class']) for index, row in df_excel.iterrows()}\n",
    "\n",
    "# Now read the CSV, update it with phenostring and category, and then write it back\n",
    "updated_csv_path = '/lustre06/project/6061810/mikekaz/CLSA/QC/DIA_pheweb_input_data.csv'  # Path for the updated CSV\n",
    "\n",
    "# Reading the original CSV\n",
    "df_csv = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Function to update phenostring and category based on the phenocode\n",
    "def update_row(row):\n",
    "    phenocode = row['phenocode']\n",
    "    if phenocode in pheno_dict:\n",
    "        row['phenostring'], row['category'] = pheno_dict[phenocode]\n",
    "    else:\n",
    "        row['phenostring'], row['category'] = None, None  # Or use some default/fallback value\n",
    "    return row\n",
    "\n",
    "# Apply the update function to each row\n",
    "df_updated = df_csv.apply(update_row, axis=1)\n",
    "\n",
    "# Write the updated DataFrame back to a new CSV\n",
    "df_updated.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV file has been saved to: {updated_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86dddb2-df48-44f2-a778-e479073e9e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CSV file has been saved to: /lustre06/project/6061810/mikekaz/CLSA/QC/DIA_pheweb_input_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the phenotypes file\n",
    "phenotypes_path = '/lustre06/project/6061810/mikekaz/CLSA/Regenie/old_files/data/Phe_DIA_Final.csv'\n",
    "df_phenotypes = pd.read_csv(phenotypes_path, sep=' ')\n",
    "\n",
    "# Prepare a dictionary to hold the counts for each phenocode\n",
    "phenocode_counts = {}\n",
    "\n",
    "# Iterate over each phenocode column (starting from the third column)\n",
    "for phenocode in df_phenotypes.columns[2:]:\n",
    "    num_cases = df_phenotypes[phenocode].value_counts().get(1, 0)\n",
    "    num_controls = df_phenotypes[phenocode].value_counts().get(0, 0)\n",
    "    num_samples = num_cases + num_controls\n",
    "    phenocode_counts[phenocode] = (num_cases, num_controls, num_samples)\n",
    "\n",
    "# Now read the existing CSV, update it, and then write it back\n",
    "final_csv_path = '/lustre06/project/6061810/mikekaz/CLSA/QC/DIA_pheweb_input_data.csv'  # Path for the final updated CSV\n",
    "\n",
    "# Reading the updated CSV\n",
    "df_csv = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Function to update num_cases, num_controls, and num_samples based on the phenocode\n",
    "def update_counts(row):\n",
    "    phenocode = row['phenocode']\n",
    "    if phenocode in phenocode_counts:\n",
    "        row['num_cases'], row['num_controls'], row['num_samples'] = phenocode_counts[phenocode]\n",
    "    else:\n",
    "        row['num_cases'], row['num_controls'], row['num_samples'] = None, None, None  # Or use some default/fallback value\n",
    "    return row\n",
    "\n",
    "# Apply the update function to each row\n",
    "df_final = df_csv.apply(update_counts, axis=1)\n",
    "\n",
    "# Write the final DataFrame back to a new CSV\n",
    "df_final.to_csv(final_csv_path, index=False)\n",
    "\n",
    "print(f\"Final CSV file has been saved to: {final_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4ba61-3b85-4204-869c-dda116e4a6df",
   "metadata": {},
   "source": [
    "## Merging two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac4df9c-957e-46e9-b2f7-49518fce4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Path to the original CSV file and the file with new rows\n",
    "original_csv_path = 'ALL_binary.csv'\n",
    "new_rows_csv_path = 'DIA_pheweb_input_data.csv'\n",
    "\n",
    "\n",
    "# Open the original CSV file in append mode\n",
    "with open(original_csv_path, mode='a', newline='') as original_file:\n",
    "    writer = csv.writer(original_file)\n",
    "    \n",
    "    # Open the new rows CSV file in read mode\n",
    "    with open(new_rows_csv_path, mode='r') as new_file:\n",
    "        reader = csv.reader(new_file)\n",
    "        \n",
    "        # Skip the header row of the new rows file\n",
    "        next(reader)\n",
    "        \n",
    "        # Initialize a counter for the rows beyond the header\n",
    "        row_counter = 0\n",
    "        \n",
    "        # Loop through each row in the new rows file\n",
    "        for row in reader:\n",
    "            # Since we only want to add the two rows after the header,\n",
    "            # we check if we've already added two rows\n",
    "            if row_counter < 2:\n",
    "                writer.writerow(row)\n",
    "                row_counter += 1\n",
    "            else:\n",
    "                break  # Stop the loop if we've already added two rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de24145-d199-4612-9133-037df1cd2112",
   "metadata": {},
   "source": [
    "## now that i have the file with all the information for each phenotype, i dont want to redo it everytime in a new dir, i just want to change the dir(assoc_files column) everytime, the rest of the columns are constant. so below, we take an input dir and just change the assoc_files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e460b8-96aa-403c-83e0-86560a5ba098",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m new_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/mikekaz/scratch/CLSA_Mike/Regenie/FINAL/Regenie_nextflow/Binary/results/step2/summary/Pheweb_ready_summary_files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m output_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALL_binary.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Name/path of the output CSV file\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mupdate_csv_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mupdate_csv_dir\u001b[0;34m(csv_file, new_dir, output_file)\u001b[0m\n\u001b[1;32m      6\u001b[0m writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(outfile)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Read the header and write it unchanged\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriterow(headers)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Update the directory path in the first column for each row and write to the output file\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "\n",
    "# def update_csv_dir(csv_file, new_dir, output_file):\n",
    "#     with open(csv_file, mode='r', newline='') as infile, open(output_file, mode='w', newline='') as outfile:\n",
    "#         reader = csv.reader(infile)\n",
    "#         writer = csv.writer(outfile)\n",
    "        \n",
    "#         # Read the header and write it unchanged\n",
    "#         headers = next(reader)\n",
    "#         writer.writerow(headers)\n",
    "        \n",
    "#         # Update the directory path in the first column for each row and write to the output file\n",
    "#         for row in reader:\n",
    "#             # Split the original path to get the filename\n",
    "#             original_path = row[0]\n",
    "#             filename = original_path.split('/')[-1]\n",
    "            \n",
    "#             # Construct the new path and update the row\n",
    "#             new_path = f\"{new_dir}/{filename}\"\n",
    "#             row[0] = new_path\n",
    "            \n",
    "#             # Write the updated row to the output file\n",
    "#             writer.writerow(row)\n",
    "            \n",
    "#     print(f\"Updated CSV has been saved to {output_file}\")\n",
    "\n",
    "# # Usage example\n",
    "# original_csv = 'ALL_binary.csv'  # Replace with your CSV file path\n",
    "# new_directory = '/home/mikekaz/scratch/CLSA_Mike/Regenie/FINAL/Regenie_nextflow/Binary/results/step2/summary/Pheweb_ready_summary_files'\n",
    "# output_csv = 'ALL_binary.csv'  # Name/path of the output CSV file\n",
    "\n",
    "# update_csv_dir(original_csv, new_directory, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30851f4-c155-40bb-a722-f6a9cf862c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def update_csv_dir(csv_file, new_dir, output_file, encoding='utf-8'):\n",
    "    # Check if the file exists and is not empty\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"The file {csv_file} does not exist.\")\n",
    "        return\n",
    "    if os.path.getsize(csv_file) == 0:\n",
    "        print(f\"The file {csv_file} is empty.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(csv_file, mode='r', newline='', encoding=encoding) as infile, open(output_file, mode='w', newline='', encoding=encoding) as outfile:\n",
    "            reader = csv.reader(infile)\n",
    "            writer = csv.writer(outfile)\n",
    "            \n",
    "            try:\n",
    "                # Read the header and write it unchanged\n",
    "                headers = next(reader)\n",
    "                writer.writerow(headers)\n",
    "            except StopIteration:\n",
    "                print(\"The CSV file appears to be empty beyond the headers.\")\n",
    "                return\n",
    "            \n",
    "            # Update the directory path in the first column for each row and write to the output file\n",
    "            for row in reader:\n",
    "                # Split the original path to get the filename\n",
    "                original_path = row[0]\n",
    "                filename = original_path.split('/')[-1]\n",
    "                \n",
    "                # Construct the new path and update the row\n",
    "                new_path = f\"{new_dir}/{filename}\"\n",
    "                row[0] = new_path\n",
    "                \n",
    "                # Write the updated row to the output file\n",
    "                writer.writerow(row)\n",
    "                \n",
    "        print(f\"Updated CSV has been saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the file: {e}\")\n",
    "\n",
    "# Usage example\n",
    "original_csv = 'ALL_binary.csv'  # Replace with your actual CSV file path\n",
    "new_directory = '/home/mikekaz/scratch/CLSA_Mike/Regenie/FINAL/Regenie_nextflow/Binary/results/step2/summary/Pheweb_ready_summary_files'\n",
    "output_csv = 'updated_ALL_binary.csv'  # Consider changing the name to prevent overwriting\n",
    "\n",
    "update_csv_dir(original_csv, new_directory, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec396b94-6f24-483c-8387-c8ba98bc3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging Justin's file\n",
    "import csv\n",
    "\n",
    "# Path to the original CSV file and the file with new rows\n",
    "original_csv_path = 'Wave1_CLSA_Pheweb_data.csv'\n",
    "new_rows_csv_path = 'CLSA_Pheweb_continuous_data.csv'\n",
    "\n",
    "# Open the original CSV file in append mode\n",
    "with open(original_csv_path, mode='a', newline='') as original_file:\n",
    "    writer = csv.writer(original_file)\n",
    "    \n",
    "    # Open the new rows CSV file in read mode\n",
    "    with open(new_rows_csv_path, mode='r') as new_file:\n",
    "        reader = csv.reader(new_file)\n",
    "        \n",
    "        # Skip the header row of the new rows file\n",
    "        next(reader)\n",
    "        \n",
    "        # Loop through each row in the new rows file and add it to the original file\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e622c-994c-415c-b30f-78865a4e656f",
   "metadata": {},
   "source": [
    "## convert to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d189773a-d626-4a31-9c27-dd120c3fa6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file was created successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "csv_file_path = 'updated_ALL_binary.csv'\n",
    "json_file_path = 'updated_ALL_binary.json'\n",
    "\n",
    "# Initialize an empty list to hold the formatted data\n",
    "data_to_export = []\n",
    "\n",
    "try:\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "        # Reading CSV file using comma as delimiter\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            # Prepare the row for JSON export, selectively excluding missing values\n",
    "            formatted_row = {}\n",
    "            for key, value in row.items():\n",
    "                # Check if the value is missing for 'num_cases' or 'num_controls', if so, skip adding it\n",
    "                if value == '' and key in ['num_cases', 'num_controls']:\n",
    "                    continue\n",
    "\n",
    "                # Reformat the 'assoc_files' field to be a list\n",
    "                if key == 'assoc_files':\n",
    "                    formatted_row[key] = [value]\n",
    "                # Convert 'num_cases', 'num_controls', and 'num_samples' to integers\n",
    "                elif key in ['num_cases', 'num_controls', 'num_samples'] and value.isdigit():\n",
    "                    formatted_row[key] = int(value)\n",
    "                # Add other values as they are\n",
    "                else:\n",
    "                    formatted_row[key] = value\n",
    "            \n",
    "            # Add the formatted row to the list of data to export\n",
    "            data_to_export.append(formatted_row)\n",
    "    \n",
    "    # Write the data to a JSON file\n",
    "    with open(json_file_path, mode='w', encoding='utf-8') as json_file:\n",
    "        json.dump(data_to_export, json_file, indent=2)\n",
    "    \n",
    "    print(\"JSON file was created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d6e58-6453-439b-9192-24bb880a061a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
